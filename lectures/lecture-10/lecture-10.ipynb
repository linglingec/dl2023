{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 10:  Contrastive learning / self-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture: General tricks for efficient training:\n",
    "- checkpointing\n",
    "- offloading\n",
    "- efficient communications \n",
    "- low-precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Current lecture: Contrastive learning / self-supervised learning\n",
    "- What is contrastive learning\n",
    "- Popular losses\n",
    "- SimCLR\n",
    "- MoCo\n",
    "- ByOL\n",
    "- Barlow Twins\n",
    "- VicReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is constrastive learning\n",
    "\n",
    "Constrastive learning is a technique, where you have a lot of **unlabeled data** (typically, it is related to images).\n",
    "\n",
    "We want to learn an **encoder** $x \\rightarrow E(x)$ such that we can distinguish **positive** and **negative** samples.\n",
    "\n",
    "Contrastive learning is a special class of self-supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constrastive learning\n",
    "\n",
    "We select a data sample (sometimes called **anchor**), the data point belonging to the same distribution as anchor,\n",
    "\n",
    "and a **negative sample**.\n",
    "\n",
    "An SSL model tries to minimize the distance between anchor and positive sample, while simultaneously maximizing the distance between positive samples and negative samples.\n",
    "\n",
    "The distance can be measured in many different ways.\n",
    "\n",
    "<img src='clr-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to find positive and negative samples?\n",
    "\n",
    "There are several methods to find positive-negative samples.\n",
    "\n",
    "There are many ways to measure **distances**.\n",
    "\n",
    "The standard approach is to apply **image transformations** to the anchor image and use those as positive samples.\n",
    "\n",
    "We can use standard augmentations covered in our class on it, typically:\n",
    "\n",
    "color jitter, rotation, flipping, noise, random affine.\n",
    "\n",
    "<img src='clr-2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image patches\n",
    "\n",
    "The second approach is to break image into **patches**. \n",
    "\n",
    "Use patches from the same image as **positive**, from others as **negative**.\n",
    "\n",
    "<img src='clr-3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "We have a query $q$, positive key $k^+$ and negative key $k^-$. We need to introduce the notions described above. \n",
    "\n",
    "This can be done by using several losses:\n",
    "\n",
    "- Max margin contrastive loss\n",
    "- Triplet loss\n",
    "- Info NCE\n",
    "- NT-Xent Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Max-margin contrastive loss\n",
    "\n",
    "The first loss was proposed by Chopra, Hadsell, and LeCun in 2006, and was motivated by so-called **energy models**.\n",
    "\n",
    "Specifically, it has the form\n",
    "\n",
    "$$ L_{\\mathrm{pair}} = \\begin{cases} D(q, k)^2, & k \\sim p^+(\\cdot \\vert q) \\\\ \\max(0, m - D(q, k)^2) &k \\sim p^-(\\cdot \\vert q) \\end{cases}$$\n",
    "\n",
    "The distance between positive pairs is minimized, between negative it is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Triplet loss\n",
    "\n",
    "Triplet loss had a strong impact in FaceId systems, where contrastive approaches are very efficient \n",
    "\n",
    "It is given as\n",
    "\n",
    "$$L(q, k^+, k^-) = \\max(0, D(q, k^+)^2 - D(q, k^-)^2 + m)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic losses\n",
    "\n",
    "$$p(k^+|q) = \\frac{\\exp(q^{\\top} k^+)}{\\sum_{k \\in \\mathcal{K}} \\exp(q^T k)} = \\frac{\\exp(q^{\\top} k^+)}{Z(q)}.$$\n",
    "\n",
    "The normalization constant is hard to evaluate: we need to sum over all negative samples in the dataset for the given query.\n",
    "\n",
    "The original NCE (noise contrastive estimation) assume a uniform distribution of the negative samples for a given query.\n",
    "\n",
    "Then, if we sample $m$ times more often the negative sample, we get the following value ($D=1$ corresponds to the positive distribution)\n",
    "$$p(D = 1|q, k) = \\frac{p(k^+ \\vert q)}{p(k^+\\vert q) + m \\cdot p(k^- \\vert q)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General scheme for constrastive learning\n",
    "\n",
    "Now we are ready to discuss the basic approaches.\n",
    "\n",
    "<img src='constrastive-learning-scheme.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First idea\n",
    "\n",
    "The first idea has been done in the paper [Dimensionality Reduction by Learning an Invariant Mapping\n",
    "Raia Hadsell, Sumit Chopra, Yann LeCun](https://www.academia.edu/download/56222713/cvpr06.pdf) in 2006, \n",
    "\n",
    "With the idea of learning representations that are invariant to certain transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Later techniques\n",
    "\n",
    "Several techniques have been proposed later.\n",
    "\n",
    "One is [Contrastive predictive coding](https://arxiv.org/abs/1807.03748) where autoregressive models are used for prediction.\n",
    "\n",
    "One can also think about **BERT**-type masking as contrastive learning, but it requires additional visual token encoders, which map images to visual tokens.\n",
    "\n",
    "We basically predict the missing part given the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different approaches\n",
    "\n",
    "There are different approaches where do we get positive/negative samples.\n",
    "\n",
    "We can have a **memory bank** that stores the negative samples, and train using it. It requires additional memory.\n",
    "\n",
    "There are alternative approaches. \n",
    "\n",
    "We will discuss the most popular ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory bank idea\n",
    "\n",
    "Instead of the softmax classifier, we consider a non-parametric version of it, where $v_i$ are **prototypes**.\n",
    "\n",
    "We will talk about prototypes when we will take about one-shot / few-shot learning.\n",
    "\n",
    "$$P(i \\vert v) = \\frac{\\exp(v^{\\top}_i v/\\tau)}{\\sum_{j=1}^n \\exp(v^{\\top}_j v/\\tau)}$$\n",
    "\n",
    "All features for each object is stored in the memory bank. When the model is updated, the features are updated as well.\n",
    "\n",
    "If we have millions of classes using full softmax is prohibited, thus NCE can be used.\n",
    "\n",
    "More discussion is in [this paper](https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SimCLR\n",
    "\n",
    "[Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning\n",
    "f visual representations](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf)\n",
    "\n",
    "SimCLR is a **framework**, which is built on a general principles from certain modules.\n",
    "\n",
    "- Stochastic **data augmentation module**: transforms any given data sample randomly, giving two different views $\\hat{x}_i$ and $\\hat{x}_j$. These are two correlated views of the same image. We consider this as a positive pair.\n",
    "- **Base encoder** (given as a neural network). Can be ResNet or transformer module.\n",
    "- A small **projection head** that maps the input to the space where the contrastive loss is applied. A simple 1-layer network is used.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SimCLR: continued\n",
    "\n",
    "Suppose we have $N$ sample. We generate $2N$ by augmentation. We think we have one positive pair and $2N-2$ negative pairs. Thus we may introduce a loss function as\n",
    "\n",
    "$$l_{ij} = -\\log\\frac{\\exp(\\text{sim}(z_i,z_{j})/\\tau)}{\\sum_{k=1}^{2N}[k\\neq i]\\exp(\\text{sim}(z_i,z_k)/\\tau)}$$\n",
    "\n",
    "where $$\\mathrm{sim}(u, v) = \\frac{(u, v)}{\\Vert u \\Vert \\Vert v \\Vert}$$\n",
    "\n",
    "is the cosine similarity between vectors $u$ and $v$.\n",
    "\n",
    "This loss has been used before (called NT-Xent).\n",
    "\n",
    "We need to sum over all pairs $i, j$.\n",
    "\n",
    "Facts from the paper: \n",
    "1) The model is trained with large batch sizes.\n",
    "2) Much stronger augmentation is needed compared to supervised learning, in the end they settled down with simple cropping and color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MoCo\n",
    "\n",
    "\n",
    "\n",
    "[Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\n",
    "Girshick. Momentum contrast for unsupervised visual representation learning.](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf)\n",
    "\n",
    "We can look at the contrastive learning as **dictionary lookup task**.\n",
    "\n",
    "\n",
    "We have an input query **q** and $\\{k_0, k_1, k_2, \\ldots, \\}$ are the encoded samples.\n",
    "\n",
    "A contrastive loss is a function that is low when $q$ is close to its positive pair $k_{+}$ are far from all other keys.\n",
    "\n",
    "MoCo uses $$L_q = -\\log \\frac{\\exp((q \\cdot k_+)/\\tau)}{\\sum_{i=0}^K \\exp((q \\cdot k_i)/\\tau)},$$\n",
    "\n",
    "the sum is over $K$ negative samples\n",
    "\n",
    "<img src='moco-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MoCo, continued\n",
    "\n",
    "The dictionary is kept large, the encoder is updated but **not trained**, but it is smoothed as \n",
    "\n",
    "$$\\theta_k = m \\theta_k + (1-m) \\theta_q,$$\n",
    "\n",
    "where $m = 0.999$ \n",
    "\n",
    "```python\n",
    "# f_q, f_k: encoder networks for query and key\n",
    "# queue: dictionary as a queue of K keys (CxK)\n",
    "# m: momentum\n",
    "# t: temperature\n",
    "\n",
    "f_k.params = f_q.params # initialize\n",
    "\n",
    "for x in loader: # load a minibatch x with N samples\n",
    "    x_q = aug(x) # a randomly augmented version\n",
    "    x_k = aug(x) # another randomly augmented version\n",
    "    q = f_q.forward(x_q) # queries: NxC\n",
    "    k = f_k.forward(x_k) # keys: NxC\n",
    "    k = k.detach() # no gradient to keys\n",
    "    \n",
    "    # positive logits: Nx1\n",
    "    l_pos = bmm(q.view(N,1,C), k.view(N,C,1))\n",
    "    # negative logits: NxK\n",
    "    l_neg = mm(q.view(N,C), queue.view(C,K))\n",
    "    # logits: Nx(1+K)\n",
    "    logits = cat([l_pos, l_neg], dim=1)\n",
    "    # contrastive loss, Eqn.(1)\n",
    "    labels = zeros(N) # positives are the 0-th\n",
    "    loss = CrossEntropyLoss(logits/t, labels)  \n",
    "    # SGD update: query network\n",
    "    loss.backward()\n",
    "    update(f_q.params)\n",
    "    # momentum update: key network\n",
    "    f_k.params = m*f_k.params+(1-m)*f_q.params\n",
    "    # update dictionary\n",
    "    enqueue(queue, k) # enqueue the current minibatch\n",
    "    dequeue(queue) # dequeue the earliest minibatch.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MoCo version 2\n",
    "\n",
    "[Improved Baselines with Momentum Contrastive Learning, by \n",
    "Xinlei Chen, Haoqi Fan, Ross Girshick and Kaiming He](https://arxiv.org/pdf/2003.04297.pdf)\n",
    "\n",
    "\n",
    "Some architectural ideas borrowed from SimCLR that improve the accuracy! \n",
    "\n",
    "1. Replace FC head with 2-layer MLP head\n",
    "2. Add blur augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap your Latent (BYOL)\n",
    "\n",
    "[Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning](https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html)\n",
    "\n",
    "In BYOL, two networks are used. They are referred to as online and target networks, that interact and learn from each other.\n",
    "\n",
    "The network weights $\\xi$ are update after each training step as\n",
    "\n",
    "$$\\xi := \\tau \\xi + (1-\\tau) \\theta).$$\n",
    "\n",
    "The motivation behind is quite vague, but the SOTA numbers were obtained!\n",
    "\n",
    "<img src='byol-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Siamese networks\n",
    "\n",
    "[Exploring Simple Siamese Representation Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf)\n",
    "\n",
    "\n",
    "One can share two networks and just update them:\n",
    "\n",
    "<img src='simsim.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Barlow twins\n",
    "\n",
    "One can also try the objective to make the representations **uncorrelated**, as done \n",
    "in [Barlow Twins: Self-Supervised Learning via Redundancy Reduction](http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf)\n",
    "\n",
    "<img src='barlow-twins.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Barlow-twins\n",
    "\n",
    "Barlow twins naturally avoids the collapse problem by requiring the two views be completely uncorrelated.\n",
    "\n",
    "I.e., we augment the data two times (two views of the dataset) and then we require that the correlation between those is close to $1$.\n",
    "\n",
    "```python\n",
    "# f: encoder network\n",
    "# lambda: weight on the off-diagonal terms\n",
    "# N: batch size\n",
    "# D: dimensionality of the embeddings\n",
    "#\n",
    "# mm: matrix-matrix multiplication\n",
    "# off_diagonal: off-diagonal elements of a matrix\n",
    "# eye: identity matrix\n",
    "for x in loader: # load a batch with N samples\n",
    "    # two randomly augmented versions of x\n",
    "    y_a, y_b = augment(x)\n",
    "    # compute embeddings\n",
    "    z_a = f(y_a) # NxD\n",
    "    z_b = f(y_b) # NxD\n",
    "    # normalize repr. along the batch dimension\n",
    "    z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0) # NxD\n",
    "    z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD\n",
    "    # cross-correlation matrix\n",
    "    c = mm(z_a_norm.T, z_b_norm) / N # DxD\n",
    "    # loss\n",
    "    c_diff = (c - eye(D)).pow(2) # DxD\n",
    "    # multiply off-diagonal elems of c_diff by lambda\n",
    "    off_diagonal(c_diff).mul_(lambda)\n",
    "    loss = c_diff.sum()\n",
    "    # optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vic-Reg\n",
    "\n",
    "Another approach proposed in [VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning](https://arxiv.org/abs/2105.04906) \n",
    "\n",
    "The goal is to avoid some special tricks with **mode collapse**.\n",
    "\n",
    "\n",
    "<img src='vicreg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VicReg: the loss\n",
    "\n",
    "There are 3 losses:\n",
    "\n",
    "**Invariance:** the mean square distance between the embedding vectors.\n",
    "\n",
    "**Variance:** a hinge loss to maintain the standard deviation (over a batch) of each variable of\n",
    "the embedding above a given threshold. This term forces the embedding vectors of samples\n",
    "within a batch to be different.*\n",
    "\n",
    "**Covariance:** a term that attracts the covariances (over a batch) between every pair of\n",
    "(centered) embedding variables towards zero. This term decorrelates the variables of each\n",
    "embedding and prevents an informational collapse in which the variables would vary\n",
    "together or be highly correlated.\n",
    "\n",
    "\n",
    "The losses are quite complicated and can be found in the [paper itself](https://arxiv.org/abs/2105.04906) \n",
    "\n",
    "A cool property is that modalities of the branches can **be different**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contrastive Whitening\n",
    "\n",
    "A very nice (but theoretically not explained!) idea has been proposed in \n",
    "[Whitening for Self-Supervised Representation Learning, Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe](http://proceedings.mlr.press/v139/ermolov21a.html)\n",
    "\n",
    "\n",
    "Again, we generate augmentations.\n",
    "\n",
    "We have positive pair $(x_i, x_j)$ and we want to learn a mapping $z = f(x, \\theta)$ such that\n",
    "\n",
    "$$E \\mathrm{dist}(z_i, z_j) \\rightarrow \\min, \\quad \\mathrm{cov}(z_i, z_j) = I.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constrastive Whitening\n",
    "\n",
    "The orthogonalization is implemented to avoid mode collapse. Note, that you need to differentiate through QR here (in the paper is implemented through regularized Cholesky decomposition)\n",
    "\n",
    "<img src='whitening.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation as spectral clustering\n",
    "\n",
    "Can we find any theoretical justification of what is going on here?\n",
    "\n",
    "The simplest case would be augmentation with Gaussian noise.\n",
    "\n",
    "Let $u$ be the mapping, and we are looking for a $1D$ embedding first.\n",
    "\n",
    "Then, we will have to minimize\n",
    "\n",
    "$$ E_{x, \\varepsilon} \\Vert u(x + \\varepsilon) - u(x) \\Vert^2. $$\n",
    "\n",
    "Let $\\varepsilon \\sim N(0, \\sigma^2)$, then this has a limit for $\\sigma \\rightarrow 0$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation as spectral clustering\n",
    "\n",
    "We will have the following optimization:\n",
    "\n",
    "$$\\int \\rho \\Vert \\nabla u \\Vert^2 dx \\rightarrow \\min, \\,  \\mbox{s.t.} \\int \\rho u^2 = 1, \\int \\rho u = 0$$\n",
    "\n",
    "This is leading eigenvalue of the weighted Laplacian operator on the manifold!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation as spectral clustering\n",
    "\n",
    "\n",
    "A general study has been done in [Contrastive and Non-Contrastive Self-Supervised\n",
    "Learning Recover Global and Local Spectral Embedding\n",
    "Methods](https://arxiv.org/pdf/2205.11508.pdf)\n",
    "\n",
    "This paper considers a simplified model, when the **views** of the dataset are linear maps.\n",
    "\n",
    "<img src='ssl-general.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Losses revisited\n",
    "\n",
    "We can now look at those losses in the [paper](https://arxiv.org/pdf/2205.11508.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More efficient training for SSL  models\n",
    "\n",
    "[Recent paper: EMP-SSL: TOWARDS SELF-SUPERVISED LEARNING IN ONE\n",
    "TRAINING EPOCH](https://arxiv.org/pdf/2304.03977.pdf)\n",
    "\n",
    "claims that we can significantly speed-up training contrastive loss framework.\n",
    "\n",
    "Originally, the methods are quite slow to train\n",
    "<img src='training-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EMP-SSL\n",
    "\n",
    "Idea is to split into really many patches! \n",
    "\n",
    "Then, enforce similarity with the mean, simultaneously avoiding collapse.\n",
    "\n",
    "<img src='emp-ssl.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contrastive learning for multimodal data\n",
    "\n",
    "There is no much difference between contrastive learning for images and images/text.\n",
    "\n",
    "The latter is even simpler, if the dataset is organized into pairs!.\n",
    "\n",
    "This is the part of the famous **CLIP model!**\n",
    "\n",
    "<img src='CLIP.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some challenges for the CLIP model\n",
    "\n",
    "The CLIP paper reports batch size 32k for training, i.e. the scaling $B^2$ for the model. \n",
    "\n",
    "A good research question is wether we really need such a large batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some insights\n",
    "- The type of data augmentation is very important\n",
    "- Very large batch size (i.e., large number of negatives) is required for contrastive loss to work well.\n",
    "- Typically, training is quite slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- What is contrastive learning\n",
    "- Popular losses\n",
    "- SimCLR\n",
    "- MoCo\n",
    "- ByOL\n",
    "- Barlow Twins\n",
    "- VicReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture: Few-shot learning/zero-shot learning/foundation models\n",
    "\n",
    "- What is few-shot learning\n",
    "- What is zero-shot learning\n",
    "- Popular datasets for few-shot/zero learning\n",
    "- Brief description of the concept of foundation models."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
