{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 14:  Generative models II (Generative adversarial models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture:  Generative models I \n",
    "- Autoregressive models \n",
    "- Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Current lecture:\n",
    "\n",
    "- Generative adversarial networks\n",
    "- DCGAN\n",
    "- WGAN\n",
    "- StyleGAN\n",
    "- Evaluating generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is GANs\n",
    "\n",
    "We are considering the problem of learning a distribution from samples.\n",
    "\n",
    "We parametrize the distribution as\n",
    "\n",
    "$$G(z), z \\sim p(z).$$\n",
    "\n",
    "So, we have access to **real samples** and **fake samples** (generated from the generator).\n",
    "\n",
    "How we distinguish between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The original idea of [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "has the following form: we treat the problem of distinguishing between **real** and **fake** samples as binary classification task.\n",
    "\n",
    "<img src='gan.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization formulation\n",
    "\n",
    "We have a **min-max problem**: we want the discriminator to detect the fakes, whereas we want the generator to **fool the discriminator**\n",
    "\n",
    "$$\\min_G \\max_D L(D, G), \\quad L(D, G) = E_{x \\sim p_{r}} \\log D(x) + E_{z \\sim p_z} \\log(1-D(G(z))$$\n",
    "\n",
    "What the solution of this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact solution of the min-max problem\n",
    "\n",
    "Suppose that real data has density $\\rho_r$ and synthetic data has density $\\rho_g$.\n",
    "\n",
    "Then, \n",
    "we have\n",
    "\n",
    "$$\\int \\rho_r(x) \\log D(x) + \\int \\rho_g(x) \\log (1 - D(x)).$$\n",
    "\n",
    "By optimizing over $D(x)$ we get\n",
    "\n",
    "$$\\frac{\\rho_r}{D} - \\frac{\\rho_g}{1-D} = 0,$$\n",
    "\n",
    "$$\\rho_g(1-D) - \\rho_rD = 0, \\quad  D = \\frac{\\rho_r}{\\rho_g + \\rho_r}$$\n",
    "\n",
    "This is the **optimal discriminator**.\n",
    "\n",
    "If we put the optimal discriminator into the formula, we get\n",
    "\n",
    "$$L(G, D_*) = \\int \\rho_r \\log \\frac{\\rho_r}{\\rho_g + \\rho_r} + \\int \\rho \\log \\frac{\\rho_g}{\\rho_g + \\rho_r}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal formula\n",
    "\n",
    "For the optimal discriminator defined before, we have\n",
    "\n",
    "$$L(G, D^*) = 2D_{JS}(p_{r} \\| p_g) - 2\\log2, $$\n",
    "\n",
    "where $D_{JS}$ is the Jensen-Shannon divergence (symmetrized version of the KL-divergence)\n",
    "\n",
    "$$D_{JS}(p \\| q) = \\frac{1}{2} D_{KL}(p \\| \\frac{p + q}{2}) + \\frac{1}{2} D_{KL}(q \\| \\frac{p + q}{2})$$\n",
    "\n",
    "Thus, minimization over $G$ will lead to $p_r = p_g$ if the generator is expressive enough, \n",
    "\n",
    "and  $D_* = \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a basic GAN\n",
    "\n",
    "In training a basic GAN,  we:\n",
    "\n",
    "1. Make a few optimizer steps in $G$\n",
    "2. Make a few optimizer step in $D$.\n",
    "\n",
    "Sometimes, in early training $G$ is quite poor and $\\log(1 - D(G(z))$ **saturates** and it has been proposed to train $G$ to maximize $\\log D(G(z))$ rather than minimize $\\log(1 - D(G(z))$.\n",
    "\n",
    "The gradients are much larger.\n",
    "\n",
    "The problem with this 'non-saturating' loss is that it is heuristic and is not well-described by theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note on non-saturating loss\n",
    "\n",
    "With the optimal $D_*$ in this case we have\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "E_{x \\sim p_g}\\left[-\\log \\left(D_G^*(x)\\right)\\right]+E_{x \\sim p_g}\\left[\\log \\left(1-D_G^*(x)\\right)\\right] \\\\\n",
    "=E_{x \\sim p_g}\\left[\\log \\frac{\\left(1-D_G^*(x)\\right)}{D_G^*(x)}\\right]=E_{x \\sim p_g}\\left[\\log \\frac{p_g(x)}{p_{\\text {data }}(x)}\\right] \\\\\n",
    "=K L\\left(p_g \\| p_{\\text {data }}\\right) .\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "After putting it all together, we will get the following loss for $G$ as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& E_{x \\sim p_g}\\left[-\\log \\left(D_G^*(x)\\right)\\right] \\\\\n",
    "& =K L\\left(p_g \\| p_{\\text {data }}\\right)-2 J S\\left(p_{\\text {data }} \\| p_g\\right) \\\\\n",
    "& +E_{x \\sim p_{\\text {data }}}\\left[\\log D_G^*(x)\\right]+2 \\log 2 . \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "$$\n",
    "The first loss minimizes the difference, while the second maximizes. This may result in unstable training for $G$.\n",
    "\n",
    "Two extreme cases:\n",
    "\n",
    "- If $p_{\\text {data }}(x) \\rightarrow 0$ and $p_g(x) \\rightarrow 1$, we have $K L\\left(p_g\\right.$ $\\left.\\| p_{\\text {data }}\\right) \\rightarrow+\\infty$.\n",
    "- If $p_{\\text {data }}(x) \\rightarrow 1$ and $p_g(x) \\rightarrow 0$, we have $K L\\left(p_g\\right.$ $\\left.\\| p_{\\text {data }}\\right) \\rightarrow 0$\n",
    "\n",
    "First error: $G$ produces implausible examples. The second error: generator produces insufficiently diverse ('safe' example). This problem is known as **mode collapse**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DCGAN\n",
    "Original GAN model involved MLP and is not scalable. \n",
    "\n",
    "[DeepConvolutional GAN](https://arxiv.org/pdf/1511.06434.pdf) was the first working GAN for large-scale image generation. It provided several recipes for building architectures and stable training, \n",
    "\n",
    "such as\n",
    "\n",
    "- Using BatchNorm\n",
    "- Using fully-convolutional layers\n",
    "- Remove pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wasserstein GAN (WGAN)\n",
    "\n",
    "[Wasserstein GAN](https://arxiv.org/abs/1701.07875) was based on the idea of using another type of metric between distribution to minimize, namely the **Wasserstein distance**.\n",
    "\n",
    "The Earth-Mover (EM) distance or Wasserstein-1\n",
    "$$\n",
    "W\\left(\\mathbb{P}_r, \\mathbb{P}_g\\right)=\\inf _{\\gamma \\in \\Pi\\left(\\mathbb{P}_r, \\mathbb{P}_g\\right)} \\mathbb{E}_{(x, y) \\sim \\gamma}[\\|x-y\\|]\n",
    "$$\n",
    "where $\\Pi\\left(\\mathbb{P}_r, \\mathbb{P}_g\\right)$ denotes the set of all joint distributions $\\gamma(x, y)$ whose marginals are respectively $\\mathbb{P}_r$ and $\\mathbb{P}_g$. Intuitively, $\\gamma(x, y)$ indicates how much \"mass\" must be transported from $x$ to $y$ in order to transform the distributions $\\mathbb{P}_r$ into the distribution $\\mathbb{P}_g$. The EM distance then is the \"cost\" of the optimal transport plan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of when Wasserstein can be better\n",
    "\n",
    "Example 1 (Learning parallel lines). Let $Z \\sim U[0,1]$ the uniform distribution on the unit interval. Let $\\mathbb{P}_0$ be the distribution of $(0, Z) \\in \\mathbb{R}^2$ (a 0 on the x-axis and the random variable $Z$ on the y-axis), uniform on a straight vertical line passing through the origin. Now let $g_\\theta(z)=(\\theta, z)$ with $\\theta$ a single real parameter. It is easy to see that in this case,\n",
    "- $W\\left(\\mathbb{P}_0, \\mathbb{P}_\\theta\\right)=|\\theta|$\n",
    "- $J S\\left(\\mathbb{P}_0, \\mathbb{P}_\\theta\\right)= \\begin{cases}\\log 2 & \\text { if } \\theta \\neq 0 \\\\ 0 & \\text { if } \\theta=0\\end{cases}$\n",
    "- $K L\\left(\\mathbb{P}_\\theta \\| \\mathbb{P}_0\\right)=K L\\left(\\mathbb{P}_0 \\| \\mathbb{P}_\\theta\\right)= \\begin{cases}+\\infty & \\text { if } \\theta \\neq 0, \\\\ 0 & \\text { if } \\theta=0,\\end{cases}$\n",
    "- and $\\delta\\left(\\mathbb{P}_0, \\mathbb{P}_\\theta\\right)= \\begin{cases}1 & \\text { if } \\theta \\neq 0 \\\\ 0 & \\text { if } \\theta=0\\end{cases}$,\n",
    "\n",
    "where $\\delta$ is a Total Variation distance, defined as \n",
    "\n",
    "$$\n",
    "\\delta\\left(\\mathbb{P}_r, \\mathbb{P}_g\\right)=\\sup _{A \\in \\Sigma}\\left|\\mathbb{P}_r(A)-\\mathbb{P}_g(A)\\right| .\n",
    "$$\n",
    "\n",
    "As we see, the only 'smooth' is the Wasserstein distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training objective of WGAN\n",
    "Computing infimum in \n",
    "\n",
    "$$\n",
    "W\\left(\\mathbb{P}_r, \\mathbb{P}_g\\right)=\\inf _{\\gamma \\in \\Pi\\left(\\mathbb{P}_r, \\mathbb{P}_g\\right)} \\mathbb{E}_{(x, y) \\sim \\gamma}[\\|x-y\\|]\n",
    "$$\n",
    "\n",
    "is intractable (or not easy at least). There is a **Kantorovich duality** representation of this distance as\n",
    "\n",
    "$$\n",
    "W\\left(\\mathbb{P}_r, \\mathbb{P}_\\theta\\right)=\\sup _{\\|f\\|_L \\leq 1} \\mathbb{E}_{x \\sim \\mathbb{P}_r}[f(x)]-\\mathbb{E}_{x \\sim \\mathbb{P}_\\theta}[f(x)]\n",
    "$$\n",
    "where the supremum is over all the 1-Lipschitz functions $f: \\mathcal{X} \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "It is also sufficient to optimize over bounded-Lipschitz functions\n",
    "\n",
    "The objective is similar to the GAN, but with the **Lipschitz constraint**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Enforcing Lipschitz constraint\n",
    "\n",
    "The original WGAN paper proposed to use weight clipping to $[-c, c]$ (and confirmed it is a terrible idea).\n",
    "\n",
    "A subsequent paper [Improved Wasserstein GAN training](https://proceedings.neurips.cc/paper_files/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html)  uses **gradient penalty** instead\n",
    "\n",
    "The resulting loss function has the form \n",
    "\n",
    "$$L=\\underbrace{\\underset{\\tilde{\\boldsymbol{x}} \\sim \\mathbb{P}_g}{\\mathbb{E}}[D(\\tilde{\\boldsymbol{x}})]-\\underset{\\boldsymbol{x} \\sim \\mathbb{P}_r}{\\mathbb{E}}[D(\\boldsymbol{x})]}_{\\text {Original critic loss }}+\\underbrace{\\lambda \\underset{\\hat{\\boldsymbol{x}} \\sim \\mathbb{P}_{\\hat{\\boldsymbol{x}}}}{\\mathbb{E}}\\left[\\left(\\left\\|\\nabla_{\\hat{\\boldsymbol{x}}} D(\\hat{\\boldsymbol{x}})\\right\\|_2-1\\right)^2\\right] .}_{\\text {Our gradient penalty }}$$\n",
    "\n",
    "and $\\hat{x}$ is sampled from straight lines between original samples and fake samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectral normalization\n",
    "\n",
    "Another way of stabilizing GAN training is [Spectral normalization](https://arxiv.org/pdf/1802.05957.pdf)\n",
    "\n",
    "The idea is to introduce spectral norm bound on the weights (by doing power iteration)\n",
    "\n",
    "Algorithm 1 SGD with spectral normalization\n",
    "- Initialize $\\tilde{\\boldsymbol{u}}_l \\in \\mathcal{R}^{d_l}$ for $l=1, \\ldots, L$ with a random vector (sampled from isotropic distribution).\n",
    "- For each update and each layer $l$ :\n",
    "1. Apply power iteration method to a unnormalized weight $W^l$ :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\tilde{\\boldsymbol{v}}_l \\leftarrow\\left(W^l\\right)^{\\mathrm{T}} \\tilde{\\boldsymbol{u}}_l /\\left\\|\\left(W^l\\right)^{\\mathrm{T}} \\tilde{\\boldsymbol{u}}_l\\right\\|_2 \\\\\n",
    "& \\tilde{\\boldsymbol{u}}_l \\leftarrow W^l \\tilde{\\boldsymbol{v}}_l /\\left\\|W^l \\tilde{\\boldsymbol{v}}_l\\right\\|_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "2. Calculate $\\bar{W}_{\\mathrm{SN}}$ with the spectral norm:\n",
    "$$\n",
    "\\bar{W}_{\\mathrm{SN}}^l\\left(W^l\\right)=W^l / \\sigma\\left(W^l\\right), \\text { where } \\sigma\\left(W^l\\right)=\\tilde{\\boldsymbol{u}}_l^{\\mathrm{T}} W^l \\tilde{\\boldsymbol{v}}_l\n",
    "$$\n",
    "3. Update $W^l$ with SGD on mini-batch dataset $\\mathcal{D}_M$ with a learning rate $\\alpha$ :\n",
    "$$\n",
    "W^l \\leftarrow W^l-\\alpha \\nabla_{W^l} \\ell\\left(\\bar{W}_{\\mathrm{SN}}^l\\left(W^l\\right), \\mathcal{D}_M\\right)\n",
    "$$\n",
    "\n",
    "Spectral normalization is present in many GAN architectures nowdays, but it is not present in StyleGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSGAN\n",
    "\n",
    "The cross-entropy is not a panacea. One can actually use different loss functions, for example [Least Squares GAN](https://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)\n",
    "\n",
    "with the min-max problem of the form\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _D V_{\\mathrm{LSGAN}}(D) & =\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}\\left[(D(\\boldsymbol{x})-b)^2\\right] \\\\\n",
    "& +\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}\\left[(D(G(\\boldsymbol{z}))-a)^2\\right] \\\\\n",
    "\\min _G V_{\\mathrm{LSGAN}}(G) & =\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}\\left[(D(G(\\boldsymbol{z}))-c)^2\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The resulting objective is smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence of the training process\n",
    "\n",
    "GANs are much more difficult to train, than standard networks due to the adversarial loss.\n",
    "\n",
    "It can be shown, that min-max problems solved with gradient descent/ascent are prone to **oscillatory behaviour**\n",
    "\n",
    "due to the presence of complex eigenvalues in the linearization of the model.\n",
    "\n",
    "Many methods have been proposed to 'monotonize' the convergence, but most of them are not very functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaptive discriminator augmentation\n",
    "\n",
    "If we train on a small dataset, the discriminator tends to overfit.\n",
    "\n",
    "\n",
    "In [Training Generative Adversarial Networks with Limited Data, by T. Karras et. al](https://proceedings.neurips.cc/paper/2020/hash/8d30aa96e72440759f74bd2306c1fa3d-Abstract.html) \n",
    "\n",
    "it has been proposed to make problem more difficult for the discriminator by **data augmentation**.\n",
    "\n",
    "Note, we need to use **differentiable data augmentation**.\n",
    "\n",
    "The strength of augmentation also has to be determined.\n",
    "\n",
    "It significantly improves on small and medium datasets!\n",
    "\n",
    "<img src='AdaArch.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics for evaluating the quality of generated images\n",
    "\n",
    "The standard metrics for evaluating the qualities of generated images are:\n",
    "\n",
    "- Inception Score\n",
    "- Frechet Inception distance (FID)\n",
    "- Precision and Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inception Score\n",
    "\n",
    "Inception score has been proposed in [Improved Techniques for Training GANs](https://proceedings.neurips.cc/paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html)\n",
    "\n",
    "The idea is to compute conditional label distribution $p(y \\mid x)$ with an Inception model. Good images are assumed to have low-entropy, quantized by the formula\n",
    "\n",
    "$$\\exp \\left(\\mathbb{E}_{\\boldsymbol{x}} \\operatorname{KL}(p(y \\mid \\boldsymbol{x}) \\| p(y))\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frechet Inception Distance\n",
    "\n",
    "By far the most popular metric, proposed in [GANs Trained by a Two Time-Scale Update Rule\n",
    "Converge to a Local Nash Equilibrium](https://arxiv.org/pdf/1706.08500.pdf)\n",
    "\n",
    "1. The images are mapped by the Inception v2. network to a latent space.\n",
    "2. In the latent space we fit two Gaussians, one for real images, another for fake images\n",
    "3. We compute the Wasserstein-2 distance between those two Gaussians by using the formula\n",
    "\n",
    "$$d^2\\left((\\boldsymbol{m}, \\boldsymbol{C}),\\left(\\boldsymbol{m}_w, \\boldsymbol{C}_w\\right)\\right)=\\left\\|\\boldsymbol{m}-\\boldsymbol{m}_w\\right\\|_2^2+\\operatorname{Tr}\\left(\\boldsymbol{C}+\\boldsymbol{C}_w-2\\left(\\boldsymbol{C} \\boldsymbol{C}_w\\right)^{1 / 2}\\right)$$\n",
    "\n",
    "Disadvantage: it only compares two momemts of the distributions, thus ignoring higher-order topological representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating higher resolution images\n",
    "\n",
    "In order to generate higher-resolution images, several approaches have been proposed.\n",
    "\n",
    "In [SAGAN (Self-attention GAN)](https://arxiv.org/abs/1805.08318) a self-attention is built into the CNN architecture.\n",
    "\n",
    "SAGAN was later scaled to [BigGAN](https://arxiv.org/abs/1809.11096) by using several engineering tricks.\n",
    "\n",
    "The next step has been progressive growing of GANS, which we will discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Progressive growing of GANS\n",
    "\n",
    "In \n",
    "[Progressive Growing of GANs for Improved Quality, Stability, and Variation\n",
    "](https://arxiv.org/abs/1710.10196) a new methodology for training GANs has been proposed.\n",
    "\n",
    "The idea is to train on lower resolutions and add layers to $G$ and $D$.\n",
    "\n",
    "Other techniques include minibatch discrimination \n",
    "\n",
    "<img src='progressive-growing.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## StyleGAN\n",
    "\n",
    "[A Style-Based Generator Architecture for Generative Adversarial Networks by T. Karras et. al.](https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html)\n",
    "\n",
    "the **StyleGAN architecture** has been proposed.\n",
    "\n",
    "The loss function and discriminator are the same, only generator (+few tricks) is redesigned to put noise in different resolutions.\n",
    "\n",
    "One of the new block is **Adaptive Normalization** given as \n",
    "\n",
    "$$\\operatorname{AdaIN}\\left(\\mathbf{x}_i, \\mathbf{y}\\right)=\\mathbf{y}_{s, i} \\frac{\\mathbf{x}_i-\\mu\\left(\\mathbf{x}_i\\right)}{\\sigma\\left(\\mathbf{x}_i\\right)}+\\mathbf{y}_{b, i}$$\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <div style=\"flex: 1; margin-right: 10px; margin-top: 20px; width: 100%\">  \n",
    "                <img src=\"stylegan.png\" width=\"100%\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin-top: 20px\">\n",
    "        <img src=\"ablation-stylegan.png\" width=\"100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## StyleGAN version 2\n",
    "\n",
    "In [Analyzing and Improving the Image Quality of StyleGAN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.pdf) the quality of generated images has improved even further.\n",
    "\n",
    "- The AdaIN operations has been removed, replaced by the weight demodulation, which is just normalization of convolutional weights.\n",
    "- Regularization term in the loss is used less frequently\n",
    "- The new architecture for the generator does not use progressive growing and uses skip connection\n",
    "\n",
    "<img src='stylegan-v2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## StyleGAN version 2: results\n",
    "\n",
    "At a single $\\mathbf{w} \\in \\mathcal{W}$ the local metric scaling properties of the generator mapping $g(\\mathbf{w}): \\mathcal{W} \\rightarrow \\mathcal{Y}$ are captured by the Jacobian matrix $\\mathbf{J}_{\\mathbf{w}}=\\delta g(\\mathbf{w}) / \\delta \\mathbf{w}$. Motivated by the desire to preserve the expected lengths of vectors regardless of the direction, we formulate the regularizer as:\n",
    "$$\n",
    "\\mathbb{E}_{\\mathbf{w}, \\mathbf{y} \\sim \\mathcal{N}(0, \\mathbf{I})}\\left(\\left\\|\\mathbf{J}_{\\mathbf{w}}^{\\mathbf{T}} \\mathbf{y}\\right\\|_2-a\\right)^2\n",
    "$$\n",
    "where $y$ are random images with normally distributed pixel intensities, and $w \\sim f(z)$, where $z$ are normally distributed.\n",
    "\n",
    "<img src='stganv2-results.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quality of generation of images\n",
    "\n",
    "<img src='GanFaceProgress.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative image2image transformations\n",
    "\n",
    "[Conditional adversarial networks](https://arxiv.org/abs/1411.1784) showed that it is quite easy to modify the generation subject to some conditioning information.\n",
    "\n",
    "Instead of $p(x)$ we just model $p(x \\mid y)$.\n",
    "\n",
    "The original paper conditioned on the class, but you can also do image2image transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image2image transformations with conditional GANs\n",
    "\n",
    "[Image-to-Image Translation with Conditional Adversarial Networks](https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf)\n",
    "\n",
    "shows how to modify GAN architecture for image-to-image translation.\n",
    "\n",
    "A conditional GAN learns a mapping from a pair $\\{x, z\\}$ to the output $y$.\n",
    "\n",
    "The objective of a conditional GAN can be expressed as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{c G A N}(G, D)= & \\mathbb{E}_{x, y}[\\log D(x, y)]+ \\\\\n",
    "& \\mathbb{E}_{x, z}[\\log (1-D(x, G(x, z))]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since we have to output something close to $y$, it is beneficial to add **reconstruction loss** to the model:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{L 1}(G)=\\mathbb{E}_{x, y, z}\\left[\\|y-G(x, z)\\|_1\\right]\n",
    "$$\n",
    "Our final objective is\n",
    "$$\n",
    "G^*=\\arg \\min _G \\max _D \\mathcal{L}_{c G A N}(G, D)+\\lambda \\mathcal{L}_{L 1}(G)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unpair image translation\n",
    "\n",
    "The approach described before requires a parallel corpus. GAN, however, learn distributions.\n",
    "\n",
    "Can we learn a mapping without having paired data?\n",
    "\n",
    "One of the seminal methods has been **cycleGAN** proposed in [Unpaired image-to-image translation\n",
    "using cycle-consistent adversarial networks](https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf).\n",
    "\n",
    "<img src='paired-unpaired.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CycleGAN\n",
    "\n",
    "GAN-loss has the standard form:\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\mathrm{GAN}}\\left(G, D_Y, X, Y\\right) & =\\mathbb{E}_{y \\sim p_{\\text {data }}(y)}\\left[\\log D_Y(y)\\right] \\\\\n",
    "& +\\mathbb{E}_{x \\sim p_{\\text {data }}(x)}\\left[\\log \\left(1-D_Y(G(x))\\right]\\right.\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "Cycle consistency loss:\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_{\\mathrm{cyc}}(G, F) & =\\mathbb{E}_{x \\sim p_{\\text {data }}(x)}\\left[\\|F(G(x))-x\\|_1\\right] \\\\\n",
    "& +\\mathbb{E}_{y \\sim p_{\\text {data }}(y)}\\left[\\|G(F(y))-y\\|_1\\right]\n",
    "\\end{aligned}\n",
    "\n",
    "Full formulation:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\\left(G, F, D_X, D_Y\\right)= & \\mathcal{L}_{\\mathrm{GAN}}\\left(G, D_Y, X, Y\\right) \\\\\n",
    "& +\\mathcal{L}_{\\mathrm{GAN}}\\left(F, D_X, Y, X\\right) \\\\\n",
    "& +\\lambda \\mathcal{L}_{\\mathrm{cyc}}(G, F)\n",
    "\\end{aligned}\n",
    "<img src='cyclegan.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple domains \n",
    "\n",
    "CycleGAN and cGAN with paired data can only learn for two domains.\n",
    "\n",
    "The approach of [StarGAN](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) conditions the generation on the domain type with a single model.\n",
    "\n",
    "The loss has 3 components: the adversarial loss, the reconstruction loss and the domain classification loss (obtain from the discriminator).\n",
    "\n",
    "StarGAN also has 2 versions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of GANs\n",
    "\n",
    "- Image Synthesis (images that do not exist, deepfakes, avatars)\n",
    "- Image2Image translation\n",
    "- Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GAN inversion\n",
    "\n",
    "One of the problem of a GAN model is that it is not **invertible** easily.\n",
    "\n",
    "There is a [big review](https://arxiv.org/pdf/2101.05278.pdf) on the subject.\n",
    "\n",
    "Given a real image $x$, it is not always possible to find $z$ such that $G(z) \\approx x$.\n",
    "\n",
    "For standard GAN, we can look for optimal $z$.\n",
    "\n",
    "For StyleGAN, additionally, we can optimize over \n",
    "\n",
    "A typical solution to this problem now for StyleGAN is to optimize over $W$ or even every style vector (named $W^{+}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretable directions\n",
    "\n",
    "The latent space of GANs has interesting structure.\n",
    "\n",
    "One can find linear direction in it which corresponds to **interpretable directions**.\n",
    "\n",
    "There are many papers on this and many way to **disentangle** such directions.\n",
    "\n",
    "One of the has been proposed by [Voynov and Babenko](https://proceedings.mlr.press/v119/voynov20a.html)\n",
    "\n",
    "<img src='voynov-babenko.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding interpretable directions\n",
    "\n",
    "We need to find directions in the latent space that are interpretable.\n",
    "\n",
    "We introduce an additional network $R$ (called reconstructor) \n",
    "\n",
    "Reconstructor receives a pair $G(z), G(z + \\varepsilon a_k)$ and tries to predict the shift.\n",
    "\n",
    "$$\\min _{A, R} \\underset{z, k, \\varepsilon}{\\mathbb{E}} L(A, R)=\\min _{A, R} \\underset{z, k, \\varepsilon}{\\mathbb{E}}\\left[L_{c l}(k, \\widehat{k})+\\lambda L_r(\\varepsilon, \\widehat{\\varepsilon})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- Generative adversarial networks\n",
    "- DCGAN\n",
    "- WGAN\n",
    "- StyleGAN\n",
    "- Evaluating generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative models III\n",
    "- Score-based models\n",
    "- Denoising diffusion models\n",
    "- Diffusion models and stochastic differential equations"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
