{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 15:  Generative models III (Score-based and diffusion models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture\n",
    "\n",
    "- Generative adversarial networks\n",
    "- DCGAN\n",
    "- WGAN\n",
    "- StyleGAN\n",
    "- Evaluating generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Current lecture\n",
    "\n",
    "- Score matching\n",
    "- Denoising diffusion models\n",
    "- Diffusion models and stochastic ordinary differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Score matching\n",
    "\n",
    "Score matching has been introduced in [Song, Ermon, Generative Modeling by Estimating Gradients of the Data Distribution\n",
    "](https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html)\n",
    "\n",
    "Fisher divergence between two densities is defines as\n",
    "\n",
    "$$E_p \\Vert \\nabla \\log q - \\nabla \\log p \\Vert^2$$\n",
    "\n",
    "In score matching, we learn the **score function** $s_{\\theta} = \\nabla \\log q$. \n",
    "\n",
    "The objective minimizes $\\frac{1}{2} \\mathbb{E}_{p_{\\text {data }}}\\left[\\left\\|\\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x})-\\nabla_{\\mathbf{x}} \\log p_{\\text {data }}(\\mathbf{x})\\right\\|_2^2\\right]$, which can be shown equivalent to the following up to a constant\n",
    "$$\n",
    "\\mathbb{E}_{p_{\\text {data }}(\\mathbf{x})}\\left[\\operatorname{tr}\\left(\\nabla_{\\mathbf{x}} \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x})\\right)+\\frac{1}{2}\\left\\|\\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x})\\right\\|_2^2\\right]\n",
    "$$\n",
    "\n",
    "The main challenge is in the evaluation of the trace of the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating the trace: denoising score matching\n",
    "\n",
    "There are two ways of approximation of the trace. The first is called **denoising score matching** and is based on the well-known approximation of the Jacobian.\n",
    "\n",
    "The objective is written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2} \\mathbb{E}_{q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) p_{\\text {data }}(\\mathbf{x})}\\left[\\left\\|\\mathbf{s}_{\\boldsymbol{\\theta}}(\\tilde{\\mathbf{x}})-\\nabla_{\\tilde{\\mathbf{x}}} \\log q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x})\\right\\|_2^2\\right]\n",
    "\\end{equation}\n",
    "\n",
    "where \\begin{equation}\n",
    "q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x})\n",
    "\\end{equation} is the predefined noise distribution and\n",
    "\n",
    "\\begin{equation}\n",
    "q_\\sigma(\\tilde{\\mathbf{x}}) \\triangleq \\int q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) p_{\\mathrm{data}}(\\mathbf{x}) \\mathrm{d} \\mathbf{x}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating the trace: sliced score matching\n",
    "We can also approximate the trace by using **Hutchinson trace estimator**. \n",
    "\n",
    "The objective reads \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{p_{\\mathbf{v}}} \\mathbb{E}_{p_{\\text {data }}}\\left[\\mathbf{v}^{\\top} \\nabla_{\\mathbf{x}} \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}) \\mathbf{v}+\\frac{1}{2}\\left\\|\\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x})\\right\\|_2^2\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The term inside can be efficiently computed using autodifferentiation, and $p_v$ is a simple distribution (i.e., normal distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Suppose we know $s_{\\theta}(x) \\approx \\nabla \\log p$.\n",
    "\n",
    "Then, how we sample from such kind of distribution?\n",
    "\n",
    "One of the classical approaches is **Langevin sampling**.\n",
    "\n",
    "If we know the gradient of the logarithm of $p$, the following procedure will lead to samples of $p$:\n",
    "\n",
    "The Langevin method recursively computes the following\n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_t=\\tilde{\\mathbf{x}}_{t-1}+\\frac{\\epsilon}{2} \\nabla_{\\mathbf{x}} \\log p\\left(\\tilde{\\mathbf{x}}_{t-1}\\right)+\\sqrt{\\epsilon} \\mathbf{z}_t,\n",
    "$$\n",
    "where $\\mathbf{z}_t \\sim \\mathcal{N}(0, I)$. The distribution of $\\tilde{\\mathbf{x}}_T$ equals $p(\\mathbf{x})$ when $\\epsilon \\rightarrow 0$ and $T \\rightarrow \\infty$, in which case $\\tilde{\\mathbf{x}}_T$ becomes an exact sample from $p(\\mathbf{x})$ under some regularity conditions. When $\\epsilon>0$ and $T<\\infty$, a Metropolis-Hastings update is needed to correct the error in the equation above, but it can often be ignored in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Original idea of diffusion models\n",
    "\n",
    "The original idea of diffusion model has been proposed in 2015 in the paper [Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\n",
    "learning using nonequilibrium thermodynamic](http://proceedings.mlr.press/v37/sohl-dickstein15.html).\n",
    "\n",
    "\n",
    "We sequentially 'noise' the distribution in order to get to the right one.\n",
    "\n",
    "This can be defined by a whole family of distrubutions.\n",
    "\n",
    "The final result may look like this\n",
    "\n",
    "<img src='diffusion-models-unconditional_image_generation-1.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical formalism\n",
    "\n",
    "We need to define:\n",
    "\n",
    "- $q(x_t \\mid x_{t-1})$\n",
    "  Known as **forward diffusion kernel**, defines the probability density function (PDF) of the image at timestep $t$ conditioned on $x_{t-1}$. Note that it typically does not have any learnable parameters.\n",
    "- $p_{\\theta}(x_{t-1} \\mid x_t)$ -- reverse diffusion kernel, the parameters $\\theta$ are **learned** using neural network architecture.\n",
    "\n",
    "<img src='diffusion-models-forwardbackward_process_ddpm-1536x259.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How the forward diffusion is defined\n",
    "\n",
    "Modern deep denoising diffusion has been introduced in [Denoising Diffusion Probabilistic Models\n",
    " by Jonathan Ho, Ajay Jain, Pieter Abbeel, Neurips2020](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html) and called **Denosing Diffusion Probabilistic Models** (DDPM).\n",
    " \n",
    " \n",
    " The forward process is parametrized as a Markov chain:\n",
    " \n",
    " \\begin{equation}\n",
    "\\begin{aligned}\n",
    "q\\left(x_1, \\ldots, x_T \\mid x_0\\right) & :=\\prod_{t=1}^T q\\left(x_t \\mid x_{t-1}\\right) \\\\\n",
    "q\\left(x_t \\mid x_{t-1}\\right) & :=\\mathcal{N}\\left(x_t ; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I\\right)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The meaning of the forward process is **extremely simple**. \n",
    "\n",
    "Can you describe it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The samples from the PDF at time $t$ can be computed as\n",
    "\n",
    "$$\n",
    "x_t=\\sqrt{1-\\beta_t} x_{t-1}+\\sqrt{\\beta_t} \\epsilon,\n",
    "$$ where $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "Parameter $\\beta_t$ is called **diffusion rate**.\n",
    "\n",
    "Note, that in order to sample $x_t$, we need to sample $x_0, \\ldots, x_{t-1}$ which can be time-consuming.\n",
    "\n",
    "Instead, we can sample from $q(x_t \\mid x_0)$ directly, since it is also a Gaussian:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "q\\left(x_t \\mid x_0\\right) & :=\\mathcal{N}\\left(x_t ; \\sqrt{\\bar{\\alpha}_t} x_{t-1},\\left(1-\\bar{\\alpha}_t\\right) I\\right) \\\\\n",
    "x_t & :=\\sqrt{\\bar{\\alpha}_t} x_0+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\alpha_t & :=1-\\beta_t \\\\\n",
    "\\bar{\\alpha}_t & :=\\prod_{s=1}^t \\alpha_s\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "In the end, $q(x_T) = N(x_t, 0, I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reverse diffusion process\n",
    "\n",
    "The reverse process is the integral over all possible pathways to arrive a data sample.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "p_\\theta\\left(x_0\\right):=\\int p_\\theta\\left(x_{0: T}\\right) d x_{1: T} \\\\\n",
    "p_\\theta\\left(\\mathbf{x}_{0: T}\\right):=p\\left(\\mathbf{x}_T\\right) \\prod_{t=1}^T p_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right), \\quad p_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right):=\\mathcal{N}\\left(\\mathbf{x}_{t-1} ; \\boldsymbol{\\mu}_\\theta\\left(\\mathbf{x}_t, t\\right), \\mathbf{\\Sigma}_\\theta\\left(\\mathbf{x}_t, t\\right)\\right)\n",
    "\\end{gathered}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training objective\n",
    "\n",
    "We can try to maximize the likelihood!\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p_\\theta\\left(x_0\\right) & :=\\int p_\\theta\\left(x_{0: T}\\right) d x_{1: T} \\\\\n",
    "L & =-\\log \\left(p_\\theta\\left(x_0\\right)\\right)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "This likelihood is intractable. So what can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use ELBO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ELBO for DDPM\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}\\left[-\\log p_\\theta\\left(\\mathbf{x}_0\\right)\\right] \\leq \\mathbb{E}_q\\left[-\\log \\frac{p_\\theta\\left(\\mathbf{x}_{0: T}\\right)}{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_0\\right)}\\right]=\\mathbb{E}_q\\left[-\\log p\\left(\\mathbf{x}_T\\right)-\\sum_{t \\geq 1} \\log \\frac{p_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right)}{q\\left(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}\\right)}\\right]=: L\n",
    "\\end{equation}\n",
    "\n",
    "It can be rewritten as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_q[\\underbrace{D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_T \\mid \\mathbf{x}_0\\right) \\| p\\left(\\mathbf{x}_T\\right)\\right)}_{L_T}+\\sum_{t>1} \\underbrace{D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0\\right) \\| p_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right)\\right)}_{L_{t-1}} \\underbrace{-\\log p_\\theta\\left(\\mathbf{x}_0 \\mid \\mathbf{x}_1\\right)}_{L_0}]\n",
    "\\end{equation}\n",
    "\n",
    "and represented as a sum:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "L_{\\mathrm{vlb}} & :=L_0+L_1+\\ldots+L_{T-1}+L_T \\\\\n",
    "L_0 & :=-\\log p_\\theta\\left(x_0 \\mid x_1\\right) \\\\\n",
    "L_{t-1} & :=D_{K L}\\left(q\\left(x_{t-1} \\mid x_t, x_0\\right) \\| p_\\theta\\left(x_{t-1} \\mid x_t\\right)\\right) \\\\\n",
    "L_T & :=D_{K L}\\left(q\\left(x_T \\mid x_0\\right) \\| p\\left(x_T\\right)\\right)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The authors then ignore $L_0$ and it is obvious we can ignore $L_T$.\n",
    "\n",
    "Thus, only $L_{t-1}$ is left.\n",
    "\n",
    "\\begin{equation}\n",
    "L_{v l b}:=L_{t-1}:=D_{K L}\\left(q\\left(x_{t-1} \\mid x_t, x_0\\right)|| p_\\theta\\left(x_{t-1} \\mid x_t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "This objective is quite natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplifying the bound\n",
    "\n",
    "The function $q(x_{t-1} \\mid x_t, x_0)$ can be written as\n",
    "\n",
    "$$\n",
    "q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0\\right)=\\mathcal{N}\\left(\\mathbf{x}_{t-1} ; \\tilde{\\boldsymbol{\\mu}}_t\\left(\\mathbf{x}_t, \\mathbf{x}_0\\right), \\tilde{\\beta}_t \\mathbf{I}\\right)\n",
    "$$\n",
    "where $\\quad \\tilde{\\boldsymbol{\\mu}}_t\\left(\\mathbf{x}_t, \\mathbf{x}_0\\right):=\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1-\\bar{\\alpha}_t} \\mathbf{x}_0+\\frac{\\sqrt{\\alpha_t}\\left(1-\\bar{\\alpha}_{t-1}\\right)}{1-\\bar{\\alpha}_t} \\mathbf{x}_t \\quad$ and $\\quad \\tilde{\\beta}_t:=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t$\n",
    "\n",
    "The authors of DDPM then set $\\Sigma$ to be $\\sigma^2 I$, simplifying everything to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "p_\\theta\\left(x_{t-1} \\mid x_t\\right)=\\mathcal{N}\\left(x_{t-1} ; \\mu_\\theta\\left(x_t, t\\right), \\Sigma_\\theta\\left(x_t, t\\right)\\right) \\\\\n",
    "\\downarrow \\\\\n",
    "p_\\theta\\left(x_{t-1} \\mid x_t\\right)=\\mathcal{N}\\left(x_{t-1} ; \\mu_\\theta\\left(x_t, t\\right), \\sigma^2 I\\right)\n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "Since the variance is constant, the KL divergence is just the distance between the mean values, leaving the following objective:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{t-1}=\\mathbb{E}_q\\left[\\frac{1}{2 \\sigma_t^2}\\left\\|\\tilde{\\boldsymbol{\\mu}}_t\\left(\\mathbf{x}_t, \\mathbf{x}_0\\right)-\\boldsymbol{\\mu}_\\theta\\left(\\mathbf{x}_t, t\\right)\\right\\|^2\\right]+C\n",
    "\\end{equation}\n",
    "\n",
    "The next step is to rewrite everything in terms of the noise:\n",
    "\n",
    "$$\n",
    "\\bar{\\mu}\\left(x_t, x_0\\right)=\\frac{1}{\\sqrt{\\overline{x_t}}}\\left(x_t-\\frac{\\beta_t}{\\sqrt{1-\\overline{\\alpha_t}}} \\epsilon_t\\right)\n",
    "$$\n",
    "Similarly, the formulation for $\\mu_\\theta\\left(x_t, t\\right)$ is set to:\n",
    "$$\n",
    "\\mu_\\theta\\left(x_t, x_0\\right)=\\frac{1}{\\sqrt{\\overline{x_t}}}\\left(x_t-\\frac{\\beta_t}{\\sqrt{1-\\overline{\\alpha_t}}} \\epsilon_\\theta\\left(x_t, t\\right)\\right)\n",
    "$$\n",
    "\n",
    "After another simplifying assumptions, they arrived at the loss function\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\text {simple }}(\\theta):=\\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}}\\left[\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_\\theta\\left(\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0+\\sqrt{1-\\bar{\\alpha}_t} \\boldsymbol{\\epsilon}, t\\right)\\right\\|^2\\right]\n",
    "\\end{equation}\n",
    "\n",
    "I.e. we just try to fit the noise given the noised image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final algorithm\n",
    "\n",
    "The algorithm for training and sampling is short and concise.\n",
    "\n",
    "<img src='algorithm-ddpm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architecture for the DDPM\n",
    "\n",
    "We have to parametrize image-to-image network $\\varepsilon_{\\theta}$ conditioned on the timestep $t$.\n",
    "\n",
    "A good choice is UNet model.\n",
    "\n",
    "The authors used UNet with Residual blocks with self-attention.\n",
    "\n",
    "**Details**\n",
    "- There are four levels in the encoder and decoder path with bottleneck blocks between them.\n",
    "- Each encoder stage comprises two residual blocks with convolutional downsampling except the last level.\n",
    "- Each corresponding decoder stage comprises three residual blocks and uses 2x nearest neighbors with convolutions to upsample the input from the previous level.\n",
    "- Each stage in the encoder path is connected to the decoder path with the help of skip connections. \n",
    "- The model uses “Self-Attention” modules at a single feature map resolution. \n",
    "- Every residual block in the model gets the inputs from the previous layer (and others in the decoder path) and the embedding of the current timestep. The timestep embedding informs the model of the input’s current position in the Markov chain.\n",
    "\n",
    "\n",
    "<img src='ddpm-architecture.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improved DDPM models\n",
    "\n",
    "[Improved Denoising Diffusion Probabilistic Models](https://proceedings.mlr.press/v139/nichol21a.html) improves over the original DDPMs. The FID score of the DDPM was great, but log-likelihood was not the best. The latter is responsible for mode collapse.\n",
    "\n",
    "It tries to empricically parametrize the variance as \n",
    "\\begin{equation}\n",
    "\\Sigma_\\theta\\left(x_t, t\\right)=\\exp \\left(v \\log \\beta_t+(1-v) \\log \\tilde{\\beta}_t\\right)\n",
    "\\end{equation}\n",
    "\n",
    "and modify the loss function as\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{hybrid}}=L_{\\text {simple }}+\\lambda L_{\\text {vlb }}\n",
    "\\end{equation}\n",
    "\n",
    "A different noise schedule:\n",
    "\\begin{equation}\n",
    "\\bar{\\alpha}_t=\\frac{f(t)}{f(0)}, \\quad f(t)=\\cos \\left(\\frac{t / T+s}{1+s} \\cdot \\frac{\\pi}{2}\\right)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Score-based diffusion modelling\n",
    "\n",
    "DDPM is a discrete process. It can be viewed as a discretization of the stochastic differential equation (SDE), as described in\n",
    "\n",
    "[Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456)\n",
    "\n",
    "We will provide a simpler and more concise derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SDE\n",
    "\n",
    "Lets start from the continous noising process:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{d} \\mathbf{x}=\\mathbf{f}(\\mathbf{x}, t) \\mathrm{d} t+\\mathbf{G}(\\mathbf{x}, t) \\mathrm{d} \\mathbf{w}\n",
    "\\end{equation}\n",
    "where $\\mathrm{d} \\mathbf{w}$ is a **Wiener process**.\n",
    "\n",
    "We can use $f(x) = -x$, $g(x) = 1$ for example. \n",
    "\n",
    "\n",
    "If we sample $x_0$ from a certain distribution $\\rho_0 = \\rho(0, x)$, it will induce the distribution $\\rho(x, t)$ at time $t$.\n",
    "\n",
    "This density will satisfy the **Fokker-Planck equation**:\n",
    "\n",
    "where $\\mathbf{f}(\\cdot, t): \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ and $\\mathbf{G}(\\cdot, t): \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d \\times d}$. The marginal probability density $p_t(\\mathbf{x}(t))$ evolves according to Kolmogorov's forward equation (Fokker-Planck equation) \n",
    "$$\n",
    "\\frac{\\partial p_t(\\mathbf{x})}{\\partial t}=-\\sum_{i=1}^d \\frac{\\partial}{\\partial x_i}\\left[f_i(\\mathbf{x}, t) p_t(\\mathbf{x})\\right]+\\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial^2}{\\partial x_i \\partial x_j}\\left[\\sum_{k=1}^d G_{i k}(\\mathbf{x}, t) G_{j k}(\\mathbf{x}, t) p_t(\\mathbf{x})\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fokker-Planck equation\n",
    "$$\n",
    "\\frac{\\partial p_t(\\mathbf{x})}{\\partial t}=-\\sum_{i=1}^d \\frac{\\partial}{\\partial x_i}\\left[f_i(\\mathbf{x}, t) p_t(\\mathbf{x})\\right]+\\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial^2}{\\partial x_i \\partial x_j}\\left[\\sum_{k=1}^d G_{i k}(\\mathbf{x}, t) G_{j k}(\\mathbf{x}, t) p_t(\\mathbf{x})\\right]\n",
    "$$\n",
    "\n",
    "The first term correspond to deterministic drift, the second -- to stochastic diffusion.\n",
    "The stochastic diffusion **can not be reversed in a simple way**.\n",
    "\n",
    "Assume now that $\\mathbf{G}$ = I.\n",
    "\n",
    "Then we have the equation\n",
    "\n",
    "$$ \\frac{\\partial p_t(\\mathbf{x})}{\\partial t} = - \\nabla (f p_t) + \\nabla^2 p_t. $$\n",
    "\n",
    "We can rewrite it as\n",
    "\n",
    "$$ \\frac{\\partial p_t(\\mathbf{x})}{\\partial t} = - \\nabla (f p_t + p_t \\nabla \\log p_t), $$\n",
    "i.e. which correponds to the **deterministic drift**\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{d} \\mathbf{x}=\\mathbf{\\hat{f}}(\\mathbf{x}, t) \\mathrm{d} t,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{f}}(\\mathbf{x}, t) = \\mathbf{f}(\\mathbf{x}, t) - \\nabla \\log p_t(x, t).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What does it mean\n",
    "\n",
    "We have a deterministic ODE that gives **the same** marginal densities as the stochastic ODE!\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{d} \\mathbf{x}=\\mathbf{\\hat{f}}(\\mathbf{x}, t) \\mathrm{d} t,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{f}}(\\mathbf{x}, t) = \\mathbf{f}(\\mathbf{x}, t) - \\nabla \\log p_t(x, t).\n",
    "\\end{equation}\n",
    "\n",
    "But this ODE can be reversed.\n",
    "\n",
    "The only thing we need to learn is the gradient  $\\nabla \\log p_t(x, t)$.\n",
    "\n",
    "We can learn it using **score matching**, since we know how to sample from $p_t$ if $\\mathbf{f}(x) = -x$ (everything will be normal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General form of the reverse SDE and computing the likelihood.\n",
    "The general form of the inverse SDE is given as \n",
    "$$\n",
    "\\mathrm{d} \\mathbf{x}=\\underbrace{\\left\\{\\mathbf{f}(\\mathbf{x}, t)-\\frac{1}{2} \\nabla \\cdot\\left[\\mathbf{G}(\\mathbf{x}, t) \\mathbf{G}(\\mathbf{x}, t)^{\\top}\\right]-\\frac{1}{2} \\mathbf{G}(\\mathbf{x}, t) \\mathbf{G}(\\mathbf{x}, t)^{\\top} \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}, t)\\right\\}}_{=: \\tilde{f}_\\theta(\\mathbf{x}, t)} \\mathrm{d} t .\n",
    "$$\n",
    "With the instantaneous change of variables formula, we can compute the loglikelihood of $p_0(\\mathbf{x})$ using\n",
    "$$\n",
    "\\log p_0(\\mathbf{x}(0))=\\log p_T(\\mathbf{x}(T))+\\int_0^T \\nabla \\cdot \\tilde{\\mathbf{f}}_{\\boldsymbol{\\theta}}(\\mathbf{x}(t), t) \\mathrm{d} t\n",
    "$$\n",
    "\n",
    "It still requires the solution of ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making diffusion models SOTA\n",
    "\n",
    "By using architecture search, [this paper](https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html) achieved SOTA for image synthesis with diffusion models.\n",
    "\n",
    "It \n",
    "- Uses Adaptive Group Normalization to add information about time in correct place\n",
    "- Proposed **classifier guidance** which includes the class information into the normalization as well.\n",
    "\n",
    "The guidance makes the density dependent on the class label $y$, and also be made for the text-to-image generation as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier guidance\n",
    "\n",
    "Prior to classifier guidance, it was not known how to generate “low temperature” samples from a diffusion model.\n",
    "\n",
    "Low-temperature means for GAN that reducing the variance of the latent variable, we get more high-quality samples (but less diverse). Instead, we can try to generate sample that are 'closer' to some predefined classes. The class is defined by a another pretrained classifier model.\n",
    "\n",
    "**Classifier guidance:**\n",
    "\n",
    "The score given as \n",
    "\\begin{equation}\\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{z}_\\lambda, \\mathbf{c}\\right) \\approx-\\sigma_\\lambda \\nabla_{\\mathbf{z}_\\lambda} \\log p\\left(\\mathbf{z}_\\lambda \\mid \\mathbf{c}\\right)\n",
    "\\end{equation}\n",
    "is modified to include the gradient of the $\\log$ likelihood of an auxiliary classifier model $p_\\theta\\left(\\mathbf{c} \\mid \\mathbf{z}_\\lambda\\right)$ as follows:\n",
    "$$\n",
    "\\tilde{\\boldsymbol{\\epsilon}}_\\theta\\left(\\mathbf{z}_\\lambda, \\mathbf{c}\\right)=\\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{z}_\\lambda, \\mathbf{c}\\right)-w \\sigma_\\lambda \\nabla_{\\mathbf{z}_\\lambda} \\log p_\\theta\\left(\\mathbf{c} \\mid \\mathbf{z}_\\lambda\\right) \\approx-\\sigma_\\lambda \\nabla_{\\mathbf{z}_\\lambda}\\left[\\log p\\left(\\mathbf{z}_\\lambda \\mid \\mathbf{c}\\right)+w \\log p_\\theta\\left(\\mathbf{c} \\mid \\mathbf{z}_\\lambda\\right)\\right],\n",
    "$$\n",
    "where $w$ is a parameter that controls the strength of the classifier guidance. This modified score $\\tilde{\\boldsymbol{\\epsilon}}_\\theta\\left(\\mathbf{z}_\\lambda, \\mathbf{c}\\right)$ is then used in place of $\\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{z}_\\lambda, \\mathbf{c}\\right)$ when sampling from the diffusion model, resulting in approximate samples from the distribution\n",
    "$$\n",
    "\\tilde{p}_\\theta\\left(\\mathbf{z}_\\lambda \\mid \\mathbf{c}\\right) \\propto p_\\theta\\left(\\mathbf{z}_\\lambda \\mid \\mathbf{c}\\right) p_\\theta\\left(\\mathbf{c} \\mid \\mathbf{z}_\\lambda\\right)^w .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier-free guidance\n",
    "\n",
    "In [paper](https://arxiv.org/pdf/2207.12598.pdf) a classifier-free guidance is proposed.\n",
    "\n",
    "It simulaneously trains a conditional and unconditional diffusion model (using a single network for that).\n",
    "\n",
    "Sampling is performed by taking a linear combination of conditional and unconditional score estimates!\n",
    "\n",
    "<img src='classifier-free guidance.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges with diffusion models\n",
    "\n",
    "- They are very **compute-intensive**: the initial implementation used 1000 steps, i.e. 1000 applications of UNet.\n",
    "- Later research reduced it to 4-10 evaluations of the score function model, based on different ideas from numerical ODE solvers.\n",
    "- Took 150-1000 V100 GPU-days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent diffusion models and democratizing the diffusion\n",
    "\n",
    "The diffusion process occurring in the pixel space is time-consuming.\n",
    "\n",
    "[Pioneering paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf) showed that you can project the input image to the latent space, learn the diffusion model in the latent space and go back!\n",
    "\n",
    "The encoder effectively downsamples the image, leaving the 2D structure.\n",
    "\n",
    "The conditioning is enforced by cross-attention of the UNet-backbone with the conditioning features!\n",
    "\n",
    "<img src='latent-diffusion.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diffusion models for other domains\n",
    "\n",
    "The diffusion model has been extended to other domains with continuous input data.\n",
    "\n",
    "It also has been [extended to the discrete domain](https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html).\n",
    "\n",
    "The noising process is replaced by mapping to uniform distribution or absorbing state.\n",
    "\n",
    "Still in infancy.\n",
    "\n",
    "<img src='discrete-diffusion.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existing large diffusion models\n",
    "\n",
    "- **Dall-E 2**: Dall-E 2 revealed in April 2022, generated even more realistic images at higher resolutions than the original Dall-E. \n",
    "- **Imagen** is Google’s May 2022, version of a text-to-image diffusion model, which is not available to the public.\n",
    "- **Stable Diffusion:** In August 2022, Stability AI released Stable Diffusion, an open-source Diffusion model similar to Dall-E 2 and Imagen. Stability AI’s released open source code and model weights, opening up the models to the entire AI community. Stable Diffusion was trained on an open dataset, using the 2 billion English label subset of the CLIP-filtered image-text pairs open dataset LAION 5b, a general crawl of the internet created by the German charity LAION.\n",
    "- **Midjourney** is another diffusion model released in July 2022 and available via API and a discord bot.\n",
    "- **Kandinsky**: Diffusion model of Sber."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- Score-based models\n",
    "- Denoising diffusion models\n",
    "- Diffusion models and stochastic differential equations"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
