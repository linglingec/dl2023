{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Modelling sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture:  CV tasks\n",
    "- object detection (R-CNN, YOLO), metrics for evaluation \n",
    "- semantic segmentation (Masked R-CNN, U-Net), instance segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "- Modelling sequences\n",
    "- Recurrent neural networks (RNN) and their variants: RNN, GRU, LSTM\n",
    "- Concept of attention\n",
    "- Transformer architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelling sequential data\n",
    "\n",
    "Suppose we have (discrete) sequence as a single sample:\n",
    "\n",
    "$$x_1, \\ldots, x_T$$\n",
    "\n",
    "**Examples**: \n",
    "\n",
    "- Natural Language Processing (NLP), where $x_i$ are **discrete tokens**\n",
    "- Time series of different kinds (sensor data, EEG data, ...)\n",
    "\n",
    "We need to have (deep) architectures that process sequential data in order to solve different **downstream tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "The difference between MLP and Recurrent Neural Networks (RNN) is that it should be able to process sequences of different length.\n",
    "\n",
    "<img src='rnn.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of RNN\n",
    "\n",
    "In simple RNN, we introduce a **hidden state** and process the input using 1 layer:\n",
    "\n",
    "**Elman network**:\n",
    "\n",
    "$$h_t = \\sigma(W_h x_t + U_h h_{t-1} + b), \\quad y_t = \\sigma(W_y h_t + b_y).$$\n",
    "\n",
    "**Jordan network**:\n",
    "\n",
    "$$h_t = \\sigma_h(W_h x_t + U_h y_{t-1} + b), \\quad y_t = \\sigma_y(W_y h_t + b_y).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "$$h_t = \\sigma(W_h h_t x_t + U_h h_{t-1} + b), \\quad y_t = \\sigma(W_y h_t + b_y).$$\n",
    "\n",
    "Suppose we want to solve **classification tasks**\n",
    "\n",
    "How do we train the model? \n",
    "\n",
    "What are possible problems?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training using unfolding\n",
    "\n",
    "If we make $k$ steps, we can represent the resulting code a DNN with $k$ layers.\n",
    "\n",
    "This process is called **unrolling**.\n",
    "\n",
    "We can then differentiate through it in a normal way.\n",
    "\n",
    "The problems are standard: although we have to learn few parameters, the gradients can vanish.\n",
    "\n",
    "One of the breakthrough solutions have been long short-term memory model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanishing gradients\n",
    "\n",
    "Since the computational block of RNN is the same, the Jacobians are the same.\n",
    "\n",
    "The gradient in backpropagation will have the form $$W^k v$$.\n",
    "\n",
    "So, if the eigenvalues of $W$ are less than $1$ we get exponential decay.\n",
    "\n",
    "If the eigenvalues are greater than $1$, we get exponential growth.\n",
    "\n",
    "If they are one (for example for orthogonal weight matrices) you can get much more stable training of RNN.\n",
    "\n",
    "(That is why orthogonal parametrization can be important!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM: introducing memory\n",
    "\n",
    "The LSTM unit has the following **multiplicative gating mechanism**\n",
    "\n",
    "\\begin{align}\n",
    "f_t &= \\sigma(W_f[x_t, h_{t-1}] + b_f) \\\\\n",
    "i_t &= \\sigma(W_i[x_t, h_{t-1}] + b_i) \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_C[x_t, h_{t-1}] + b_C) \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\\\\n",
    "o_t &= \\sigma(W_o[x_t, h_{t-1}] + b_o) \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t)\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "- $x_t$ is the input at time step $t$\n",
    "- $h_{t-1}$ is the hidden state at time step $t-1$\n",
    "- $f_t$, $i_t$, $o_t$ are the forget gate, input gate, and output gate vectors respectively\n",
    "- $\\tilde{C}_t$ is the cell input vector\n",
    "- $C_t$ is the cell state at time step $t$\n",
    "- $\\odot$ represents element-wise multiplication\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $W_f$, $W_i$, $W_C$, $W_o$ are weight matrices for each gate and cell input\n",
    "- $b_f$, $b_i$, $b_C$, $b_o$ are bias vectors for each gate and cell input.\n",
    "\n",
    "<img src='lstm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gated recurrent unit\n",
    "\n",
    "GRU (Gated recurrent unit) is a simpler version of LSTM that has been introduced in 2014. It shows similar performancce.\n",
    "\n",
    "\\begin{align}\n",
    "r_t &= \\sigma(W_r[x_t, h_{t-1}] + b_r) \\\\\n",
    "z_t &= \\sigma(W_z[x_t, h_{t-1}] + b_z) \\\\\n",
    "\\tilde{h}_t &= \\tanh(W_h[x_t, r_t \\odot h_{t-1}] + b_h) \\\\\n",
    "h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "- $x_t$ is the input at time step $t$\n",
    "- $h_{t-1}$ is the hidden state at time step $t-1$\n",
    "- $r_t$ and $z_t$ are the reset gate and update gate vectors respectively\n",
    "- $\\tilde{h}_t$ is the candidate activation vector\n",
    "- $h_t$ is the updated hidden state at time step $t$\n",
    "- $\\odot$ represents element-wise multiplication\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $W_r$, $W_z$, $W_h$ are weight matrices for each gate and candidate activation\n",
    "- $b_r$, $b_z$, $b_h$ are bias vectors for each gate and candidate activation.\n",
    "\n",
    "<img src='gru.png' width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other types of RNN\n",
    "\n",
    "- Bi-direction RNN\n",
    "- Second-order RNN\n",
    "- More complexicated neural networks as a building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to train RNN-type models.\n",
    "\n",
    "One of the main challenges in sequence modelling is to predict the next symbol (vector) in the sequence.\n",
    "\n",
    "**Extrapolation** or prediction is one of the most important and challenging tasks.\n",
    "\n",
    "This can be viewed as **seq2seq** problem: given $x_1, \\ldots, x_T$ we want a model to output $x_2, \\ldots, x_{T+1}$.\n",
    "\n",
    "A widely used loss in this context is Connectionist temporal classification (CTC) loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CTC loss\n",
    "\n",
    "CTC loss is quite complicated loss, if we need to map a sequence of one length to the sequence of another length.\n",
    "We add additional 'blank' symbol as an output, use softmax to process RNN/LSTM data and need to compute the sum over all possible alignments.\n",
    "\n",
    "There a lot of alignments, so dynamic programming needs to be used.\n",
    "\n",
    "\n",
    "\n",
    "$$\\mathcal{L}_{\\text{CTC}}=-\\sum_{\\pi \\in \\mathcal{B}(y)}p(\\pi|x)$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{L}_{\\text{CTC}}$ is the CTC loss\n",
    "- $\\mathcal{B}(y)$ is the set of all possible alignments of the target sequence $y$\n",
    "- $p(\\pi|x)$ is the probability of alignment $\\pi$ given the input sequence $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept of attention\n",
    "\n",
    "When processing the sequence, all the information about the previous states is summarized in the **hidden state**.\n",
    "\n",
    "This may not be enough, and we want to have an opportunity to look at the previous elements in the sequence **when we need it**.\n",
    "\n",
    "A natural idea is to consider similar patterns in the sequence, which might help prediction.\n",
    "\n",
    "Of course, we can just use extended history, but it is typically not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept of attention (2)\n",
    "\n",
    "Attention has been first introduced in 2015 in the paper by Bengio et. al on Neural Nachine Translation\n",
    "\n",
    "The attention weights were a function \n",
    "\n",
    "$$e_{tj} = a(h_t, s_{t-1}), \\quad \\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\sum_{k=1}^T \\exp(e_{tk})}$$\n",
    "\n",
    "<img src='attention-lstm.webp' width=60%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention is all you need\n",
    "A seminar paper has been **attention is all you need** which greatly simplified original architecture of RNN-type models with attention and also introduced the concept of **self-attention**.\n",
    "\n",
    "In self-attention, we start from a sequence $X$ parametrized by a matrix of size $N_{seq} \\times N_{f}$, where $N_{seq}$ is the length of the sequence, and $N_f$ is the dimension of the vector space representing the sequence.\n",
    "\n",
    "We need to compute similarities between the vectors in the sequence. \n",
    "\n",
    "To do so, we map $X$ to **query** and **key** values by using linear transformations:\n",
    "\n",
    "$$Q = X W_Q, K = X W_K.$$\n",
    "\n",
    "The similarity between the $i$-th element of the sequence and the $j$-th element of the sequence is then given as a scalar product between corresponding columns:\n",
    "\n",
    "\n",
    "$$\\hat{M}_{ij} = (q_i, k_j).$$\n",
    "\n",
    "In order to make these attention weights, we use softmax operation along the columns, so\n",
    "\n",
    "$$M = \\mathrm{softmax}\\left({\\frac{QK^{\\top}}{\\sqrt{d_k}}}\\right).$$\n",
    "\n",
    "The attention matrix has the properties $M_{ij} \\geq 0, \\sum_j M_{ij} = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-attention\n",
    "\n",
    "In self-attention block, the input sequence $X$ is being mapped to three matrices $Q, K, V$ (called query, key and value) and the transformation of $X$ to the output is given by \n",
    "\n",
    "$$Q = X W_Q, \\, K = X W_K, \\, V = X W_V.$$\n",
    "\n",
    "\n",
    "$$\\mathrm{Attention(Q, K, V)} = \\mathrm{softmax}\\left({\\frac{QK^{\\top}}{\\sqrt{d_k}}}\\right) V,$$\n",
    "\n",
    "which is just taking linear combinations of vectors in $V$ with the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Designing the transformer model\n",
    "\n",
    "The attention block allows us to **exchange information** between each component of a sequence.\n",
    "\n",
    "Then, we can process the information along the feature dimension using MLP.\n",
    "\n",
    "Originally, people tried to use convolution, but ended up using MLP, but in some of the code historically we have Conv1D block.\n",
    "\n",
    "Also, we use not one **attention head**, but many of smaller dimensions.\n",
    "\n",
    "If we have H heads, we compute $H$ attention matrices.\n",
    "\n",
    "The resulting output has the same size, since $Q_i, K_i$ have smaller number of rows.\n",
    "\n",
    "Thus, we use **multihead attention** (why?) and feedforward MLP to process the features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer model\n",
    "\n",
    "The encoder-decoder architecture looks like this:\n",
    "\n",
    "<img src='transformer-enc.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comments\n",
    "\n",
    "- The architecture starts from the embedding block. Since the main application of transformers is NLP, we map discrete sequence elements from the vocabulary to the embedding vectors.\n",
    "- We also use **positional encoding**: i.e., without it the result will be invariant to permuations. The positional encoding is added to each embedding vectors. There are many types of positional encoding.\n",
    "- A transformer block contains: a) multihead attention b) MLP block\n",
    "- MLP block has two feedforward layers. The first one increases the dimension by a factor of $4$, then second one brings the dimension down, i.e. this is a **reversed bottleneck** block. It also has skip-connections and **layer normalization**.\n",
    "- We repeat the blocks as many times as we want.\n",
    "- The parameters of the transformer model are number of blocks, number of heads, the dimension of embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training transformer models\n",
    "\n",
    "For seq2seq (encoder-decoder models) we can just convert the embeddings to the probabilities of the symbol and then maximize the likelihood of the sequence.\n",
    "\n",
    "However, one of the most important applications of transformer models is **unsupervised learning**: we do not have any parallel corpus. \n",
    "\n",
    "Lets decribe the GPT (Generalized pretrained transformer) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPT model\n",
    "\n",
    "In GPT model, we have the data, which each sample being a sequence $x_1, \\ldots, x_T$. We want to learn the probability distribution\n",
    "\n",
    "$$p(x_1, \\ldots, x_T)$$\n",
    "\n",
    "We parametrize this distribution in the autoregressive form (typically it is referred to as autoregressive language modelling)\n",
    "\n",
    "$$p(x_1, \\ldots, x_T) = p(x_1) p(x_2 \\vert x_1) p(x_3 \\vert x_1, x_2) \\ldots, $$\n",
    "\n",
    "i.e. we want to learn **conditional probabilities** of the next symbol given all the previous ones.\n",
    "\n",
    "This gives the standard objective of the GPT: predict the probability of the next symbol given all the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting probabilities of the next symbol\n",
    "\n",
    "\n",
    "For the efficient training, we parametrize all conditional probabilities using a single Transformer model. \n",
    "\n",
    "We process the input data $x_1, \\ldots, x_T$ by several transformer layers, get embeddings $h_1, \\ldots, h_T$.\n",
    "\n",
    "We use **masked attention**: at each self-attention step we multiply the mask by a matrix $L$, where \n",
    "\n",
    "$$L_{ij} = \\begin{cases} 1, \\quad i \\geq j, \\\\ 0, \\mbox{otherwise}\\end{cases}.$$\n",
    "\n",
    "This means, that the $j$-th embedding depends only on the previous ones!!\n",
    "\n",
    "Thus, we can interpret those embeddings (after a linear layer) as logits for conditional probabilities.\n",
    "\n",
    "I.e. we model conditional probabilities by trying to map $x_1, \\ldots, x_T$ to $x_2, \\ldots, x_{T+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of the GPT model\n",
    "\n",
    "In **training**, we predict the conditional probabilities of the next symbol, and maximize the likelihood of the total sequence.\n",
    "\n",
    "In inference, we only have the chance to generate the next symbol (token) one-by-one, which is one of the bottlenecks of efficient generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPT model family\n",
    "\n",
    "- The GPT model architecture (GPT-1, GPT-2, GPT-3, GPT-4) does not change\n",
    "- GPT-1: 768 as embeddings, 3072 feedforward size, 12 layers, 12 heads\n",
    "- GPT-2: Larger dataset and more parameters + instructions in natural language, 48 layers, 1600 -- embedding, 50000+ vocabulary. Dataset scraped from Reddit and upvoted articles. Context length 1024\n",
    "- GPT-3: Even larger dataset and more parameters, 96 layers, 12888 embedding, 2048 context length. Largest model was 175B.\n",
    "- GPT-4 (don't know), but much larger context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different types of transformer models\n",
    "\n",
    "- GPT is an **decoder-only** model, which generates the data but does not compute meaningful embeddings of the text.\n",
    "\n",
    "- Original Neural Machine Translation models where encoder-decoder modes; now you can implement encoder-decoder models by using **trapezoidal attention mechanism**.\n",
    "\n",
    "- There are also pure **encoder models** with the most famoust being BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## BERT model\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is an **encoder-only model** trained in a **self-supervised way**.\n",
    "\n",
    "We mask some of the tokens, and then predict them.\n",
    "\n",
    "\n",
    "<img src='bert.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next-Sentence-prediction\n",
    "\n",
    "Another loss is **next sentence prediction**: the model receives two sentences and tries to predict if the next sentence is continuation of the first one.\n",
    "\n",
    "[CLS] token is inserted before the first sentence and [SEP] token is inserted between two sentences.\n",
    "\n",
    "\n",
    "<img src='nsp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT summary\n",
    "\n",
    "- We train on Masked reconstruction and next-sentence-prediction loss\n",
    "- As a result we get **good encodings** \n",
    "- These encodings can be used in downstream tasks\n",
    "- One of the main applications is **search**: we can map documents to $512$-dimensional vectors. The search is then just **nearest neighbours search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8617352843284607,\n",
       "  'token': 2515,\n",
       "  'token_str': 'does',\n",
       "  'sequence': 'artificial intelligence does not take over the world.'},\n",
       " {'score': 0.06553470343351364,\n",
       "  'token': 2097,\n",
       "  'token_str': 'will',\n",
       "  'sequence': 'artificial intelligence will not take over the world.'},\n",
       " {'score': 0.026606494560837746,\n",
       "  'token': 2106,\n",
       "  'token_str': 'did',\n",
       "  'sequence': 'artificial intelligence did not take over the world.'},\n",
       " {'score': 0.010828700847923756,\n",
       "  'token': 2064,\n",
       "  'token_str': 'can',\n",
       "  'sequence': 'artificial intelligence can not take over the world.'},\n",
       " {'score': 0.009001809172332287,\n",
       "  'token': 2071,\n",
       "  'token_str': 'could',\n",
       "  'sequence': 'artificial intelligence could not take over the world.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#import transformers\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Artificial Intelligence [MASK] take over the world.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder-decoder models\n",
    "\n",
    "There are architectures that try to combine encoder and decoder, for example, BART which combines bidirectional and autoregressive transformers:\n",
    "\n",
    "We can do transformations to the input.\n",
    "\n",
    "<img src='bart.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Huggingface\n",
    "\n",
    "https://huggingface.co/ hosts a huge amount of NLP transformer models and many different pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory and complexity\n",
    "\n",
    "\n",
    "Attention scales like $L^2$ where $L$ is the sequence length. For very long sequences it will be time consuming, so quite a long of **fast attention** variants have been proposed. \n",
    "\n",
    "For language models, the attention is not the main bottleneck.\n",
    "\n",
    "Most of the parameters are located in **feedforward layers** that map $N_f \\rightarrow 4 N_f$.\n",
    "\n",
    "They need to be compressed, quantized, etc.\n",
    "\n",
    "Recent **leaks** of LLAMA and related models, people try to quantize them but do not systematically check the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Classical autoregressive models\n",
    "- Concept of attention\n",
    "- Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture: Vision Transformer models\n",
    "\n",
    "- Original idea of VIT\n",
    "- Different applications of VIT to classical image tasks\n",
    "- Modern trends: MobileVIT, CrossVIT\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
