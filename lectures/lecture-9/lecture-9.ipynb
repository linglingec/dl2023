{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 9: Training large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture: General tricks for efficient training:\n",
    "- Data augmentation  (AutoAugment, RandAugment)\n",
    "- Label smoothing (CutMix, MixUp, ...)\n",
    "- Self-training (NoisyStudent, PseudoMetaLabels)\n",
    "- Gradient clipping \n",
    "- multi-precision training\n",
    "- Software (ML composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Current lecture: Training of large deep models\n",
    "- checkpointing\n",
    "- offloading\n",
    "- efficient communications \n",
    "- low-precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models are getting larger and larger, and requiring larger compute\n",
    "\n",
    "Also, the computational trends are quite interesting, and can be splitted into 3 different eras.\n",
    "\n",
    "The analysis is taken from [here](https://epochai.org/blog/compute-trends)\n",
    "<div style=\"display: flex\">\n",
    "    <div style=\"flex: 1; margin-right:10px; width: 100%\">  \n",
    "        \n",
    "1. **The Pre-Deep Learning Era:**  Prior to Deep Learning, training compute approximately follows Mooreâ€™s Law, with a doubling time of approximately every **20 months**.\n",
    "2. **The Deep Learning Era:** This starts somewhere between 2010 and 2012, and displays a doubling time of approximately 6 months.\n",
    "3. **The Large-Scale Era:** Arguably, a separate trend of of models breaks off the main trend between 2015 and 2016. These systems are characteristic in that they are run by large corporations, and use training compute 2-3 orders of magnitude larger than systems that follow the Deep Learning Era trend in the same year. Interestingly, the growth of compute in these Large-Scale models seems slower, with a doubling time of about 10 months.\n",
    "        \n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin-top: 20px\">\n",
    "        <img src=\"training-compute.png\" width=\"80%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why larger models\n",
    "\n",
    "- Large models show better performance (GPT-1, GPT-2, GPT-3)\n",
    "- Single models for multimodal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory constraints when training large models\n",
    "\n",
    "Large models do not fit to a GPU memory; \n",
    "\n",
    "A rule of thumb is that for M parameters we need 12M bytes\n",
    "\n",
    "12 = 4 bytes x 3 optimizer states\n",
    "\n",
    "Activations take (0.1 - 10) x number of parameters.\n",
    "\n",
    "Without offloading/checkpointing maximum is 2 billion on a V100 GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checkpointing\n",
    "\n",
    "For a backward pass, we need to store activations!\n",
    "They consume **0.1 - 10x** of the memory of the model (depending on the batch size)\n",
    "\n",
    "<img src='checkpoint.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where the memory goes\n",
    "\n",
    "Unfortunately, this is the intrinsic property of the backward pass: we need to store intermediate computations.\n",
    "\n",
    "There were recent papers on 'forward propagation' but they just implement random search.\n",
    "\n",
    "https://arxiv.org/pdf/2202.08587.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we can reduce the memory?\n",
    "\n",
    "\n",
    "Checkpointing aims at saving **part of the activations** and recomputing the rest.\n",
    "\n",
    "It add computations **but** it reduces the amount of time we need.\n",
    "\n",
    "This technique is called **rematerialization** or **activation checkpointing**.\n",
    "<img src='checkpoint.png'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checkpointing\n",
    "\n",
    "How to do the optimal checkointing? I.e. which activations to save or load?\n",
    "\n",
    "1. For a general computational graph, this is an NP-complete problem.\n",
    "2. For linear graph we have **dynamical programming**.\n",
    "3. Efficient realization is available in the [ROTOR package](https://gitlab.inria.fr/hiepacs/rotor). \n",
    "\n",
    "We are now trying to do the same thing for transformers, but it is much more complicated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another memory reduction technique: gradient accumulation\n",
    "\n",
    "\n",
    "**Standard SGD:** make a gradient for a batch, and update the weights.\n",
    "\n",
    "If the batch is large, the intermediate computations will not fit into the memory.\n",
    "\n",
    "Instead, we do the cycle of the batch and sum the result. Memory consumption is much smaller, the result is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DeepSpeed framework\n",
    "\n",
    "One of the most popular and efficient frameworks for training large models is **DeepSpeed**.\n",
    "\n",
    "\n",
    "https://github.com/microsoft/DeepSpeed\n",
    "\n",
    "For example, [Zero-Offload](https://www.deepspeed.ai/tutorials/zero-offload/) part implements **offloading**: parts of the models and part of the data is stored in the CPU memory, which is typically much larger, than the GPU memory.\n",
    "\n",
    "Optimal offloading strategy is again an **NP-complete** thing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What else can we do?\n",
    "\n",
    "Besides rematerialization and offloading, we can you good old **paralllel computations**.\n",
    "\n",
    "We split the work between different GPUs.\n",
    "\n",
    "There are different types of parallelism:\n",
    "\n",
    "1. Data parallelism: split the input batch into sub-batches\n",
    "2. Model parallelism: split the parameters of the model between different computational nodes\n",
    "3. Pipeline parallelism: minimize communication in forward & backward passes\n",
    "4. Tensor parallelism: split the feature dimension between different GPUs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data parallelism\n",
    "\n",
    "The classical approach, implemented in software, is **data parallelism**\n",
    "\n",
    "Each computational unit holds a copy of the model, processes its own batch and aggregates the gradients.\n",
    "\n",
    "This is equivalent to large batch; \n",
    "\n",
    "It also requires scatter-gather operation.\n",
    "\n",
    "What do you think are main challenges in **data parallelism**?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges in data parallelism\n",
    "\n",
    "There are two main challenges in data parallelism.\n",
    "\n",
    "1. Total communication costs\n",
    "\n",
    "2. The larger batch can lead to worser convergence of SGD!! (Lets discuss why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large-batch training\n",
    "\n",
    "Large batch training is solved by **learning rate schedule**. Although not used too much, optimizers such as LARS and LAMB\n",
    "\n",
    "[Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)\n",
    "\n",
    "<img src='lars-lamb.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training BERT in 76 minutes\n",
    "\n",
    "<img src='lars-conv.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Communication in data parallelism\n",
    "\n",
    "We need to send a lot of data: we need to have $P$ copies of the model, so quite a lot of computations.\n",
    "\n",
    "How we can parallelise the computations instead?\n",
    "\n",
    "As usual in parallel computations, we need to think about good ways of **splitting the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ZERO (DeepSpeed)\n",
    "\n",
    "The DeepSpeed frameworks implements different splitting of the data between computational nodes.\n",
    "\n",
    "\n",
    "The weights of the model, the gradients and the optimizer parameters (do not need to be forgotten) can be split among all processors.\n",
    "\n",
    "Then, (non-trivial) communication scheme has to be derived.\n",
    "\n",
    "\n",
    "<img src='zero-1.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipeline parallelism\n",
    "\n",
    "One of the challenges is that in feedforward networks with the model weights split, the computations are **sequential** and **difficult to parallelism**. \n",
    "\n",
    "Can you come with an idea how to do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipeline parallelism\n",
    "\n",
    "The solution is known as **pipeline parallelism** and is again --- NP-complete for a general computational graph.\n",
    "\n",
    "Many heuristics have been proposed, including PipeDream and GPipe.\n",
    "\n",
    "A typical solutions involves splitting the **mini-batch** into **micro-batches** and inteleaving computations of the forward/backward pass with computations. An example solution is shown on the picture.\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <div style=\"flex: 1; margin-right:10px; width: 100%\">  \n",
    "                <img src=\"pipe-stupid.png\" width=\"100%\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin-top: 20px\">\n",
    "        <img src=\"pipe-better.png\" width=\"100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "There are actually better solutions (and no optimal ones!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor parallelism \n",
    "\n",
    "Finally, if the layers are very large, \n",
    "\n",
    "one can start splitting single weights and parallelise them along different GPUs.\n",
    "\n",
    "Believed to be **last resort**.\n",
    "\n",
    "It can actually might not be true!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other techniques for faster training\n",
    "\n",
    "- Low-bit optimization\n",
    "- Gradient compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8-bit Adam\n",
    "\n",
    "[8-bit Adam paper](https://github.com/TimDettmers/bitsandbytes) showed that during Adam optimization we can store **optimizer states** within 8-bits.\n",
    "\n",
    "This significantly reduces the amount of memory you need to use!\n",
    "\n",
    "You can also use [8-bit matrix multiplication](https://arxiv.org/pdf/2208.07339.pdf) at inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient compression \n",
    "\n",
    "One of the techniques used for DALLE-2 training was [**PowerSGD**](https://proceedings.neurips.cc/paper/2019/hash/d9fbed9da256e344c1fa46bb46c34c5f-Abstract.html). They idea of power SGD is to consider the \n",
    "\n",
    "$$N \\times B$$ gradient matrix (which is split along the processors and has to be summed over $B$) \n",
    "\n",
    "and approximate it with low-rank approximation.\n",
    "\n",
    "The low-rank approximation is done by a simple **block power iterations**\n",
    "\n",
    "One can also use 1-bit Adam/SGD by replacing the gradient by it sign. Then for communication you need 16-32 times less number of bits to store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PowerSGD: algorithm\n",
    "\n",
    "The algorithm has the following form:\n",
    "\n",
    "<img src='power-sgd.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantization of the gradients\n",
    "\n",
    "One can also compute the gradient in backpropagation very inaccurately.\n",
    "\n",
    "- We can compress the gradient  for the activations\n",
    "- We can compress the gradients for the linear layers\n",
    "- We can compute the multi-head self-attention more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges in training large models\n",
    "\n",
    "THe main challenge for training large models is that even for DeepSpeed \n",
    "\n",
    "frameworks the efficiency is typically 30-60% of the peak performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary (1) \n",
    "\n",
    "- Vanilla Pytorch/jax works badly with memory. Memory consumption could be reduced and large models fine-tuned even on the GPUs with large memory.\n",
    "- Parallelism is not optimal, but data-parallelism & pipeline parallelism can be implemented and used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "- checkpointing\n",
    "- offloading\n",
    "- efficient communications \n",
    "- low-precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture: Contrastive learning / self-supervised learning\n",
    "- What is contrastive learning\n",
    "- Siamese Networks \n",
    "- Triplet loss\n",
    "- popular contrastive learning techniques"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Ð¡Ð»Ð°Ð¹Ð´-ÑˆÐ¾Ñƒ",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
