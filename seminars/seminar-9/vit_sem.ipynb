{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT from scratch\n",
    "\n",
    "by Talgat Daulbaev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation for the seminar\n",
    "\n",
    "- Probably, „transformer“ is one of the most frequent word in the lecture\n",
    "- [Transformers are the best idea in AI](https://www.youtube.com/watch?v=9uw3F6rndnA&ab_channel=LexClips) by [Andrej Karpathy](https://karpathy.ai/)\n",
    "- [Paperswithcode](https://paperswithcode.com/sota/image-classification-on-imagenet) for image classification\n",
    "- [„How to achieve success in a machine learning PhD“](https://kidger.site/thoughts/just-know-stuff/) by Patrick Kidger:\n",
    "> ...Write your own implementation of multihead attention...\n",
    "\n",
    "Today we are going to write our own implementation of <b>ViT</b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One-head scaled dot-product self-attention\n",
    "\n",
    "- Each sample is a variable length sequence of $D$-dimensional features: $x_1, x_2, \\ldots, x_{n_i}$\n",
    "- Thus, each sample can be viewed as a matrix $X \\in \\mathbb{R}^{n_i \\times d}$\n",
    "- Parameters: $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times d}$\n",
    "- Compute:\n",
    "    - $Q = X W_Q$ of size $n_i \\times d$\n",
    "    - $K = X W_K$ of size $n_i \\times d$\n",
    "    - $V = X W_V$ of size $n_i \\times d$\n",
    "    - $$\\text{Attention}(Q, K, V) = \\text{softmax_for_rows}\\left(\\dfrac{Q K^\\top}{\\sqrt{d}}\\right) V$$   \n",
    "- You can additionally normalize Q, K matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention is a „soft dictionary“\n",
    "\n",
    "- {key1: value1, key2: value2, ...}\n",
    "- Query is key that you are searching for\n",
    "- Imagine that $q_i$ and $k_j$ are normalized: $\\|q_i\\| = \\|k_j\\| = 1$\n",
    "- Then, $\\langle q_i, k_j \\rangle = \\cos(\\theta)$, where $\\theta$ is an angle between $q_i$ and $k_j$ — cosine similarity\n",
    "- We are looking at the similarity measure between $q_i$ and all key vectors and compute a linear combination of values\n",
    "$$\\text{softmax_for_rows}\\left(\\dfrac{Q K^\\top}{\\sqrt{d}}\\right) V$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task] One-head scaled dot-product self-attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax_for_rows}\\left(\\dfrac{Q K^\\top}{\\sqrt{d}}\\right) V$$ \n",
    "\n",
    "## bit.ly/vit_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args: \n",
    "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        '''\n",
    "        # Your code is here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Solution] One-head scaled dot-product self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args: \n",
    "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        '''\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.dim)\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        # ...or q, k, v = [qkv[:, :, idx, :] for idx in range(3)]\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = attn @ v\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(11, 12, 8)\n",
    "assert Attention(8)(x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-head attention\n",
    "\n",
    "\n",
    "![](Attention.png)\n",
    "\n",
    "- Divide each vector in a sequence into `num_heads` vectors ($d$ mod `num_heads` = 0)\n",
    "- Apply attention layers independently, concatenate the result\n",
    "$$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)$$\n",
    "$$ \\textrm{concat} \\left( \\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h \\right) $$\n",
    "- Apply an extra linear layer to mix independent attention branches\n",
    "- **How to implement without loops?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-head attention\n",
    "\n",
    "![](Attention.png)\n",
    "\n",
    "**Q:** How to implement without loops?\n",
    "\n",
    "**A:** Use a single QKV-linear layer, then reshape and slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task] Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        if dim % num_heads:\n",
    "            raise ValueError('dim % num_heads != 0')\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args: \n",
    "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        '''\n",
    "        # Hint: you might want to use torch.permute function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Solution] Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        if dim % num_heads:\n",
    "            raise ValueError('dim % num_heads != 0')\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args: \n",
    "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        '''\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        # qkv: 3 × B × num_heads × N × head_dim\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = attn @ v  # attn: B × num_heads × N × N    v: B × num_heads × N × head_dim\n",
    "        # B × num_heads × N × head_dim\n",
    "        x = x.transpose(1, 2).reshape(B, N, C) \n",
    "        # B × N × (num_heads × head_dim)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 12, 128])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(128, 8)(torch.ones(11, 12, 128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer Block\n",
    "\n",
    "![](vit.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task] Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4,  # ratio between hidden_dim and input_dim in MLP\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Your code is here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code is here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Solution] Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4,  # ratio between hidden_dim and input_dim in MLP\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads=num_heads)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(dim, dim * mlp_ratio), \n",
    "                                 act_layer(), \n",
    "                                 nn.Linear(dim * mlp_ratio, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 12\n",
    "many_layers = nn.Sequential(*[Block(128, 8) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Einsum\n",
    "\n",
    "Let's learn `torch.einsum` by example\n",
    "\n",
    "$$c_{n} = \\sum_{i, j} a_{n i j} b_{n j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9909, 15.4934,  3.8673, 10.1980, -8.3178,  2.1868, -3.9169, -3.2118,\n",
       "        -4.8607,  2.9071])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(10, 5, 7)\n",
    "B = torch.randn(10, 7)\n",
    "C = torch.einsum('nij,nj->n', A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9909, 15.4933,  3.8673, 10.1980, -8.3178,  2.1868, -3.9169, -3.2118,\n",
       "        -4.8607,  2.9071])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_loop = torch.zeros(10)\n",
    "for n in range(10):\n",
    "    for i in range(5):\n",
    "        for j in range(7):\n",
    "            C_loop[n] += A[n, i, j] * B[n, j]\n",
    "C_loop\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Einsum examples\n",
    "\n",
    "- Transposition: `torch.einsum('ij->ji', A)`\n",
    "- Scalar product: `torch.einsum('i,i->i', A, B)`\n",
    "- Matrix product: `torch.einsum('ik,kj->ij', A, B)`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Einops.rearrange\n",
    "\n",
    "https://github.com/arogozhnikov/einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install einops -q\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 4, 2])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposition:\n",
    "rearrange(torch.arange(1024).reshape(2, 4, 8, 16), 'aa b c d -> d c b aa').shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rearrange(torch.arange(30).reshape(5, 6), 'a (b c) -> a b c', b=2, c=3)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(res[0], res[0].flatten().reshape(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Average Pooling with einops.rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "img = torch.randn(3, 32, 32)\n",
    "blocks = rearrange(img, 'c (h h_patch) (w w_patch) -> (c h w) (h_patch w_patch)', h_patch=2, w_patch=2)\n",
    "avgpool = torch.mean(blocks, dim=-1).reshape(3, img.shape[1] // 2, img.shape[2] // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(avgpool, torch.nn.functional.avg_pool2d(img, kernel_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can also be done via `einops.reduce` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import einops\n",
    "img = torch.randn(3, 32, 32)\n",
    "avgpool = einops.reduce(img, 'c (h h_patch) (w w_patch) -> c h w', h_patch=2, w_patch=2, reduction='mean')\n",
    "torch.allclose(avgpool, torch.nn.functional.avg_pool2d(img, kernel_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task] Patches crafting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install einops -q\n",
    "from einops import rearrange\n",
    "\n",
    "def img2patches(img, patch_size=8):\n",
    "    '''\n",
    "    Args:\n",
    "        img: (batch_size, c, h, w) Tensor\n",
    "        \n",
    "    Returns:\n",
    "        (batch_size, num_patches, vectorized_patch) Tensor\n",
    "    '''\n",
    "    # Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Solution] Patches crafting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1089, 192])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def img2patches(img, patch_size=8):\n",
    "    '''\n",
    "    Args:\n",
    "        img: (batch_size, c, h, w) Tensor\n",
    "        \n",
    "    Returns:\n",
    "        (batch_size, num_patches, vectorized_patch) Tensor\n",
    "    '''\n",
    "    return rearrange(img, 'batch_size c (h ph) (w pw) -> batch_size (h w) (c ph pw)', \n",
    "                     ph=patch_size, pw=patch_size)\n",
    "    \n",
    "    \n",
    "img2patches(torch.ones(2, 3, 264, 264)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](vit.webp)\n",
    "\n",
    "- CLS token: an extra learnable token\n",
    "- Position embeddings: `x = x + pos_embedding`, where `pos_embedding` is trained for every element is a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task] Build ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "                    self,\n",
    "                    img_size=(224, 224),\n",
    "                    patch_size=16,\n",
    "                    in_chans=3,\n",
    "                    num_classes=10,\n",
    "                    embed_dim=768,\n",
    "                    depth=12,\n",
    "                    num_heads=12,\n",
    "                    mlp_ratio=4,\n",
    "                    class_token=True,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    act_layer=nn.GELU\n",
    "            ):\n",
    "        # Your code is here\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args: \n",
    "            x: (batch_size, in_channels, img_size[0], img_size[1])\n",
    "            \n",
    "        Return:\n",
    "            (batch_size, num_classes) probabilities\n",
    "        '''\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "                    self,\n",
    "                    img_size=(224, 224),\n",
    "                    patch_size=16,\n",
    "                    in_chans=3,\n",
    "                    num_classes=10,\n",
    "                    embed_dim=768,\n",
    "                    depth=12,\n",
    "                    num_heads=12,\n",
    "                    mlp_ratio=4,\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    act_layer=nn.GELU\n",
    "            ):\n",
    "        # Your code is here\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(embed_dim, num_heads, mlp_ratio, act_layer, norm_layer) for _ in range(depth)\n",
    "        ])\n",
    "        self.patch_proj = nn.Linear(3 * patch_size * patch_size, embed_dim)\n",
    "        self.embed_len = (img_size[0] * img_size[1]) // (patch_size * patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.embed_len, embed_dim) * .02)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args: \n",
    "            x: (batch_size, in_channels, img_size[0], img_size[1])\n",
    "            \n",
    "        Return:\n",
    "            (batch_size, num_classes)\n",
    "        '''\n",
    "        x = img2patches(x, patch_size=self.patch_size)\n",
    "        x = self.patch_proj(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.blocks(x)\n",
    "        x = x[:, 0, :]  # take CLS token\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViT()(torch.ones(5, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practical Notes\n",
    "- Almost all popular CV models are implemented in [timm](https://github.com/huggingface/pytorch-image-models)\n",
    "- There is a fast implementation of the multi-head attention in [torch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
